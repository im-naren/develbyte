"use strict";(self.webpackChunkmy_blog=self.webpackChunkmy_blog||[]).push([[1356],{2921:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"modern-web-development-trends-2024","metadata":{"permalink":"/develbyte/blog/modern-web-development-trends-2024","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-01-20-modern-web-development-trends.md","source":"@site/blog/2024-01-20-modern-web-development-trends.md","title":"Modern Web Development Trends in 2024","description":"The web development landscape continues to evolve rapidly, with new frameworks, tools, and practices emerging constantly. In this post, we\'ll explore the key trends shaping modern web development in 2024.","date":"2024-01-20T00:00:00.000Z","tags":[{"inline":false,"label":"Web Development","permalink":"/develbyte/blog/tags/web-development","description":"Articles about modern web development, frameworks, and best practices"},{"inline":false,"label":"JavaScript","permalink":"/develbyte/blog/tags/javascript","description":"JavaScript tips, tricks, and advanced concepts"},{"inline":false,"label":"React","permalink":"/develbyte/blog/tags/react","description":"React development, hooks, and ecosystem"},{"inline":false,"label":"TypeScript","permalink":"/develbyte/blog/tags/typescript","description":"TypeScript features, patterns, and best practices"},{"inline":false,"label":"Trends","permalink":"/develbyte/blog/tags/trends","description":"Technology trends and industry insights"}],"readingTime":3.21,"hasTruncateMarker":true,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","page":{"permalink":"/develbyte/blog/authors/narendra"},"socials":{"github":"https://github.com/im-naren","linkedin":"https://www.linkedin.com/in/im-naren/","email":"mailto:naren.dubey@zoho.com"},"imageURL":"https://github.com/im-naren.png","key":"narendra"}],"frontMatter":{"slug":"modern-web-development-trends-2024","title":"Modern Web Development Trends in 2024","authors":["narendra"],"tags":["web-development","javascript","react","typescript","trends"]},"unlisted":false,"nextItem":{"title":"Getting Started with Docusaurus - Building Your First Blog","permalink":"/develbyte/blog/getting-started-with-docusaurus"}},"content":"The web development landscape continues to evolve rapidly, with new frameworks, tools, and practices emerging constantly. In this post, we\'ll explore the key trends shaping modern web development in 2024.\\n\\n\x3c!-- truncate --\x3e\\n\\n## 1. Full-Stack Frameworks Take Center Stage\\n\\nFull-stack frameworks are becoming increasingly popular as they provide a unified approach to building web applications:\\n\\n### Next.js 14+ Features\\n- **App Router**: Revolutionary routing system with nested layouts\\n- **Server Components**: Reduced client-side JavaScript bundle\\n- **Turbopack**: Lightning-fast bundler for development\\n\\n```typescript\\n// Example: Next.js App Router with Server Components\\nexport default async function BlogPost({ params }: { params: { slug: string } }) {\\n  const post = await getPost(params.slug);\\n  \\n  return (\\n    <article>\\n      <h1>{post.title}</h1>\\n      <div dangerouslySetInnerHTML={{ __html: post.content }} />\\n    </article>\\n  );\\n}\\n```\\n\\n### Other Notable Frameworks\\n- **SvelteKit**: Compile-time optimizations\\n- **Astro**: Content-focused with minimal JavaScript\\n- **Remix**: Web standards-first approach\\n\\n## 2. TypeScript Becomes the Standard\\n\\nTypeScript adoption continues to grow, with many teams making it their default choice:\\n\\n### Benefits\\n- **Better Developer Experience**: IntelliSense and error catching\\n- **Improved Maintainability**: Self-documenting code\\n- **Enhanced Refactoring**: Safe code changes\\n\\n### Best Practices\\n```typescript\\n// Use strict type definitions\\ninterface User {\\n  id: string;\\n  email: string;\\n  profile: {\\n    name: string;\\n    avatar?: string;\\n  };\\n}\\n\\n// Leverage utility types\\ntype PartialUser = Partial<User>;\\ntype UserEmail = Pick<User, \'email\'>;\\n```\\n\\n## 3. Performance Optimization Focus\\n\\nWeb performance remains a critical concern:\\n\\n### Core Web Vitals\\n- **Largest Contentful Paint (LCP)**: < 2.5s\\n- **First Input Delay (FID)**: < 100ms\\n- **Cumulative Layout Shift (CLS)**: < 0.1\\n\\n### Optimization Techniques\\n```javascript\\n// Code splitting with dynamic imports\\nconst LazyComponent = lazy(() => import(\'./LazyComponent\'));\\n\\n// Image optimization\\nimport Image from \'next/image\';\\n\\n<Image\\n  src=\\"/hero.jpg\\"\\n  alt=\\"Hero image\\"\\n  width={800}\\n  height={600}\\n  priority\\n  placeholder=\\"blur\\"\\n/>\\n```\\n\\n## 4. AI Integration in Development\\n\\nArtificial Intelligence is transforming how we build web applications:\\n\\n### AI-Powered Tools\\n- **GitHub Copilot**: Code completion and generation\\n- **ChatGPT**: Code review and debugging assistance\\n- **V0**: UI component generation\\n\\n### Practical Applications\\n```typescript\\n// AI-generated API route handler\\nexport async function POST(request: Request) {\\n  const { prompt } = await request.json();\\n  \\n  // AI-powered content generation\\n  const response = await openai.chat.completions.create({\\n    model: \\"gpt-4\\",\\n    messages: [{ role: \\"user\\", content: prompt }],\\n  });\\n  \\n  return Response.json({ content: response.choices[0].message.content });\\n}\\n```\\n\\n## 5. Micro-Frontend Architecture\\n\\nBreaking down monolithic frontends into smaller, manageable pieces:\\n\\n### Benefits\\n- **Team Autonomy**: Independent development and deployment\\n- **Technology Diversity**: Different teams can use different frameworks\\n- **Scalability**: Easier to scale individual components\\n\\n### Implementation Patterns\\n```typescript\\n// Module Federation with Webpack\\nconst ModuleFederationPlugin = require(\'@module-federation/webpack\');\\n\\nmodule.exports = {\\n  plugins: [\\n    new ModuleFederationPlugin({\\n      name: \'shell\',\\n      remotes: {\\n        \'user-app\': \'userApp@http://localhost:3001/remoteEntry.js\',\\n        \'product-app\': \'productApp@http://localhost:3002/remoteEntry.js\',\\n      },\\n    }),\\n  ],\\n};\\n```\\n\\n## 6. Enhanced Developer Experience\\n\\nTools and practices that improve developer productivity:\\n\\n### Modern Development Tools\\n- **Vite**: Lightning-fast build tool\\n- **Turborepo**: Monorepo management\\n- **Storybook**: Component development environment\\n\\n### Development Workflow\\n```json\\n{\\n  \\"scripts\\": {\\n    \\"dev\\": \\"vite\\",\\n    \\"build\\": \\"tsc && vite build\\",\\n    \\"preview\\": \\"vite preview\\",\\n    \\"test\\": \\"vitest\\",\\n    \\"lint\\": \\"eslint . --ext ts,tsx --report-unused-disable-directives --max-warnings 0\\"\\n  }\\n}\\n```\\n\\n## 7. Security-First Development\\n\\nSecurity considerations are becoming integral to the development process:\\n\\n### Best Practices\\n- **Content Security Policy (CSP)**: Prevent XSS attacks\\n- **HTTPS Everywhere**: Encrypt all communications\\n- **Dependency Scanning**: Regular security audits\\n\\n```typescript\\n// Security headers middleware\\nexport function securityHeaders(req: Request, res: Response, next: NextFunction) {\\n  res.setHeader(\'X-Content-Type-Options\', \'nosniff\');\\n  res.setHeader(\'X-Frame-Options\', \'DENY\');\\n  res.setHeader(\'X-XSS-Protection\', \'1; mode=block\');\\n  res.setHeader(\'Strict-Transport-Security\', \'max-age=31536000; includeSubDomains\');\\n  next();\\n}\\n```\\n\\n## Looking Ahead\\n\\nThe web development ecosystem continues to evolve, with these trends shaping the future:\\n\\n1. **Edge Computing**: Bringing computation closer to users\\n2. **WebAssembly**: High-performance web applications\\n3. **Progressive Web Apps**: Native app-like experiences\\n4. **Accessibility**: Inclusive design becoming standard\\n\\n## Conclusion\\n\\nStaying current with web development trends is essential for building modern, efficient, and user-friendly applications. By embracing these trends and continuously learning, developers can create better experiences for users while improving their own productivity.\\n\\nThe key is to focus on fundamentals while selectively adopting new technologies that provide real value to your projects and users.\\n\\n---\\n\\n*What trends are you most excited about? Share your thoughts in the comments or reach out on [Twitter](https://twitter.com/narendra_kumar)!*"},{"id":"getting-started-with-docusaurus","metadata":{"permalink":"/develbyte/blog/getting-started-with-docusaurus","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-01-15-getting-started-with-docusaurus.md","source":"@site/blog/2024-01-15-getting-started-with-docusaurus.md","title":"Getting Started with Docusaurus - Building Your First Blog","description":"Docusaurus is a powerful static site generator that makes it easy to create documentation sites and blogs. In this post, we\'ll explore how to set up your first Docusaurus blog and customize it to match your brand.","date":"2024-01-15T00:00:00.000Z","tags":[{"inline":false,"label":"Docusaurus","permalink":"/develbyte/blog/tags/docusaurus","description":"Docusaurus setup, customization, and tips"},{"inline":false,"label":"Blog","permalink":"/develbyte/blog/tags/blog","description":"Blog-related content and updates"},{"inline":false,"label":"Web Development","permalink":"/develbyte/blog/tags/web-development","description":"Articles about modern web development, frameworks, and best practices"},{"inline":false,"label":"Tutorial","permalink":"/develbyte/blog/tags/tutorial","description":"Step-by-step guides and tutorials"}],"readingTime":2.8,"hasTruncateMarker":true,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","page":{"permalink":"/develbyte/blog/authors/narendra"},"socials":{"github":"https://github.com/im-naren","linkedin":"https://www.linkedin.com/in/im-naren/","email":"mailto:naren.dubey@zoho.com"},"imageURL":"https://github.com/im-naren.png","key":"narendra"}],"frontMatter":{"slug":"getting-started-with-docusaurus","title":"Getting Started with Docusaurus - Building Your First Blog","authors":["narendra"],"tags":["docusaurus","blog","web-development","tutorial"]},"unlisted":false,"prevItem":{"title":"Modern Web Development Trends in 2024","permalink":"/develbyte/blog/modern-web-development-trends-2024"},"nextItem":{"title":"Speculative Execution in Hadoop","permalink":"/develbyte/blog/speculative-execution-in-hadoop"}},"content":"Docusaurus is a powerful static site generator that makes it easy to create documentation sites and blogs. In this post, we\'ll explore how to set up your first Docusaurus blog and customize it to match your brand.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Why Choose Docusaurus?\\n\\nDocusaurus offers several advantages for developers looking to create a blog or documentation site:\\n\\n- **React-based**: Built on React, allowing for rich interactive components\\n- **Markdown support**: Write content in Markdown with MDX support\\n- **SEO optimized**: Built-in SEO features and meta tags\\n- **Responsive design**: Mobile-first approach with responsive layouts\\n- **Plugin ecosystem**: Extensive plugin system for customization\\n- **Versioning**: Built-in support for documentation versioning\\n\\n## Setting Up Your Project\\n\\nGetting started with Docusaurus is straightforward. Here\'s how to create a new project:\\n\\n```bash\\nnpx create-docusaurus@latest my-blog classic --typescript\\ncd my-blog\\nnpm start\\n```\\n\\nThis command creates a new Docusaurus project with:\\n- TypeScript support\\n- Classic template with blog functionality\\n- Development server running on `http://localhost:3000`\\n\\n## Customizing Your Blog\\n\\n### 1. Updating Configuration\\n\\nThe main configuration file is `docusaurus.config.ts`. Here are some key settings to customize:\\n\\n```typescript\\nconst config: Config = {\\n  title: \'Your Blog Name\',\\n  tagline: \'Your tagline here\',\\n  url: \'https://your-domain.com\',\\n  baseUrl: \'/\',\\n  \\n  // ... other configuration\\n};\\n```\\n\\n### 2. Styling and Theming\\n\\nCustomize your blog\'s appearance by modifying `src/css/custom.css`:\\n\\n```css\\n:root {\\n  --ifm-color-primary: #2563eb;\\n  --ifm-color-primary-dark: #1d4ed8;\\n  --ifm-color-primary-darker: #1e40af;\\n  --ifm-color-primary-darkest: #1e3a8a;\\n  --ifm-color-primary-light: #3b82f6;\\n  --ifm-color-primary-lighter: #60a5fa;\\n  --ifm-color-primary-lightest: #93c5fd;\\n}\\n```\\n\\n### 3. Creating Blog Posts\\n\\nBlog posts are written in Markdown and placed in the `blog/` directory. Each post should include frontmatter:\\n\\n```markdown\\n---\\nslug: your-post-slug\\ntitle: Your Post Title\\nauthors: [author-name]\\ntags: [tag1, tag2]\\n---\\n\\n# Your Post Title\\n\\nYour content goes here...\\n```\\n\\n## Advanced Features\\n\\n### MDX Support\\n\\nDocusaurus supports MDX, allowing you to use React components in your Markdown:\\n\\n```jsx\\nimport {CodeBlock} from \'@site/src/components/CodeBlock\';\\n\\n# My Post\\n\\n<CodeBlock language=\\"javascript\\">\\nconsole.log(\'Hello, Docusaurus!\');\\n</CodeBlock>\\n```\\n\\n### Custom Pages\\n\\nCreate custom pages by adding React components to `src/pages/`:\\n\\n```typescript\\nimport React from \'react\';\\nimport Layout from \'@theme/Layout\';\\n\\nexport default function CustomPage(): JSX.Element {\\n  return (\\n    <Layout title=\\"Custom Page\\" description=\\"A custom page\\">\\n      <div className=\\"container\\">\\n        <h1>Custom Page</h1>\\n        <p>This is a custom page built with React!</p>\\n      </div>\\n    </Layout>\\n  );\\n}\\n```\\n\\n## Deployment\\n\\nDocusaurus sites can be deployed to various platforms:\\n\\n### GitHub Pages\\n\\n```bash\\nnpm run build\\nnpm run deploy\\n```\\n\\n### Netlify\\n\\n1. Connect your repository to Netlify\\n2. Set build command: `npm run build`\\n3. Set publish directory: `build`\\n\\n### Vercel\\n\\n```bash\\nnpm install -g vercel\\nvercel\\n```\\n\\n## Best Practices\\n\\n1. **Consistent naming**: Use kebab-case for file names and slugs\\n2. **SEO optimization**: Include proper meta descriptions and titles\\n3. **Image optimization**: Use appropriate image formats and sizes\\n4. **Mobile-first**: Ensure your design works well on mobile devices\\n5. **Performance**: Optimize your site for fast loading times\\n\\n## Conclusion\\n\\nDocusaurus is an excellent choice for developers who want to create a professional blog or documentation site. With its React-based architecture, extensive customization options, and built-in features, it provides everything you need to get started quickly.\\n\\nWhether you\'re documenting a project, sharing technical insights, or building a personal brand, Docusaurus offers the flexibility and power to create exactly what you need.\\n\\nHappy blogging! \ud83d\ude80\\n\\n---\\n\\n*Have questions about Docusaurus? Feel free to reach out on [GitHub](https://github.com/narendra-kumar) or [LinkedIn](https://linkedin.com/in/narendra-kumar).*"},{"id":"speculative-execution-in-hadoop","metadata":{"permalink":"/develbyte/blog/speculative-execution-in-hadoop","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2019-04-15-speculative-execution-in-hadoop.md","source":"@site/blog/2019-04-15-speculative-execution-in-hadoop.md","title":"Speculative Execution in Hadoop","description":"Speculative execution is an optimization technique where a computer system tries to predict the slowness the task execution and run a backup task the slowness could be caused by reasons like hardware degradation or wrong configurations. since it\'s just a prediction and computer cant be sure that the task might take longer time than expected it is called speculative execution. If the prediction turns out to be wrong and the task completes before the backup task, the system reverts most changes made by the backup task and the results are ignored. if the backup task completes the fist the results from the first task is ignored.","date":"2019-04-15T00:00:00.000Z","tags":[{"inline":false,"label":"Big Data","permalink":"/develbyte/blog/tags/big-data","description":"Big data technologies and concepts"},{"inline":false,"label":"Hadoop","permalink":"/develbyte/blog/tags/hadoop","description":"Apache Hadoop ecosystem"},{"inline":false,"label":"Optimization","permalink":"/develbyte/blog/tags/optimization","description":"Performance optimization techniques"}],"readingTime":2.27,"hasTruncateMarker":false,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","page":{"permalink":"/develbyte/blog/authors/narendra"},"socials":{"github":"https://github.com/im-naren","linkedin":"https://www.linkedin.com/in/im-naren/","email":"mailto:naren.dubey@zoho.com"},"imageURL":"https://github.com/im-naren.png","key":"narendra"}],"frontMatter":{"slug":"speculative-execution-in-hadoop","title":"Speculative Execution in Hadoop","authors":["narendra"],"tags":["big-data","hadoop","optimization"],"date":"2019-04-15T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Getting Started with Docusaurus - Building Your First Blog","permalink":"/develbyte/blog/getting-started-with-docusaurus"},"nextItem":{"title":"Troubleshooting in Redshift","permalink":"/develbyte/blog/troubleshooting-in-redshift"}},"content":"Speculative execution is an optimization technique where a computer system tries to predict the slowness the task execution and run a backup task the slowness could be caused by reasons like hardware degradation or wrong configurations. since it\'s just a prediction and computer cant be sure that the task might take longer time than expected it is called speculative execution. If the prediction turns out to be wrong and the task completes before the backup task, the system reverts most changes made by the backup task and the results are ignored. if the backup task completes the fist the results from the first task is ignored.\\n\\nThe objective of speculative execution is to provide more concurrency if extra resources are available. This approach is employed in a variety of areas, including branch prediction in pipelined processors, value prediction for exploiting value locality, prefetching memory and files, and optimistic concurrency control in database systems.\\n\\nIn Hadoop, MapReduce breaks jobs into tasks and these tasks run parallel on multiple nodes, which reduces overall execution time. This model of execution can slow down the overall execution of a job if few of the task doesn\'t deliver the result at the expected time. It could be really tough to be able to detect causes since the tasks still complete successfully, although more time is taken than the expected time. \\n\\nHadoop as a system doesn\'t try to fix slow running tasks, instead, it tries to detect them and runs backup tasks for them. These backup tasks are called Speculative tasks in Hadoop.\\n\\n## How speculative execution works in Hadoop?\\n\\nFirstly all the tasks for the job are launched in Hadoop MapReduce. The speculative tasks are launched for those tasks that have been running for some time (at least one minute) and have not made any much progress, on average, as compared with other tasks from the job. The speculative task is killed if the original task completes before the speculative task, on the other hand, the original task is killed if the speculative task finishes before it.\\n\\n## Is speculative execution beneficial?\\n\\nHadoop MapReduce Speculative execution is beneficial in some cases because in a Hadoop cluster with 100s of nodes, problems like hardware failure or network congestion are common and running parallel or duplicate task would be better since we won\'t be waiting for the task in the problem to complete.\\n\\nBut if two duplicate tasks are launched at about same time, it will be a wastage of cluster resources\\n\\nHere are the two properties to configure the use of this feature:\\n\\n```\\nmapred.map.tasks.speculative.execution\\nmapred.reduce.tasks.speculative.execution\\n```\\n\\nOr if you are using Hadoop 2.x:\\n\\n```\\nmapreduce.map.speculative\\nmapreduce.reduce.speculative\\n```\\n\\nMost time it is useful but in some scenarios disabling it will make a big difference."},{"id":"troubleshooting-in-redshift","metadata":{"permalink":"/develbyte/blog/troubleshooting-in-redshift","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2019-04-15-troubleshooting-in-redshift.md","source":"@site/blog/2019-04-15-troubleshooting-in-redshift.md","title":"Troubleshooting in Redshift","description":"Redshift is a one of the most popular data warehousing solution, thousands of companies running millions of ETL jobs everyday. The problem with MPP systems is troubleshooting why the jobs are hung, which are the queries blocking others.","date":"2019-04-15T00:00:00.000Z","tags":[{"inline":false,"label":"Big Data","permalink":"/develbyte/blog/tags/big-data","description":"Big data technologies and concepts"},{"inline":false,"label":"Amazon Redshift","permalink":"/develbyte/blog/tags/redshift","description":"Amazon Redshift data warehouse"},{"inline":false,"label":"AWS","permalink":"/develbyte/blog/tags/aws","description":"Amazon Web Services"},{"inline":false,"label":"Database","permalink":"/develbyte/blog/tags/database","description":"Database design and management"}],"readingTime":4.69,"hasTruncateMarker":false,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","page":{"permalink":"/develbyte/blog/authors/narendra"},"socials":{"github":"https://github.com/im-naren","linkedin":"https://www.linkedin.com/in/im-naren/","email":"mailto:naren.dubey@zoho.com"},"imageURL":"https://github.com/im-naren.png","key":"narendra"}],"frontMatter":{"slug":"troubleshooting-in-redshift","title":"Troubleshooting in Redshift","authors":["narendra"],"tags":["big-data","redshift","aws","database"],"date":"2019-04-15T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Speculative Execution in Hadoop","permalink":"/develbyte/blog/speculative-execution-in-hadoop"},"nextItem":{"title":"Spell-correct in Python - Part 1","permalink":"/develbyte/blog/spell-correct-in-python-part-1"}},"content":"Redshift is a one of the most popular data warehousing solution, thousands of companies running millions of ETL jobs everyday. The problem with MPP systems is troubleshooting why the jobs are hung, which are the queries blocking others. \\n\\nTo troubleshoot problems like this could be a real nightmare if you are new to Redshift, in this article I have tried to aggregate the tables and queries you should always keep handy if you work with redshift on daily basis of planning to start using  \\n\\n## Table Overview\\n\\nFirst of all lets familiarize our self  with some of the tables needed to troubleshoot a problem.\\n\\n**[STV_RECENTS](https://docs.aws.amazon.com/redshift/latest/dg/r_STV_RECENTS.html)** - This table holds information about currently active and recently run queries against a database\\n\\n```sql\\nselect \\n\\tuser_name, \\n\\tdb_name, \\n\\tpid, \\n\\tquery\\nfrom stv_recents\\nwhere status = \'Running\';\\n```\\n\\n`status = \'Running\'` gives all the queries whose execution have not completed. it includes the queries which are currently executing and the queries currently waiting in the execution queue.\\n\\n**[STV_INFLIGHT](https://docs.aws.amazon.com/redshift/latest/dg/r_STV_INFLIGHT.html)** - Check the `stv_inflight` table, To find which queries are currently in progress. \\n\\n```sql\\nselect \\n\\tuserid, \\n\\tquery, \\n\\tpid, \\n\\tstarttime, \\n\\tleft(text, 50) as text \\nfrom stv_inflight;\\n```\\n\\n**[STV_LOCKS](https://docs.aws.amazon.com/redshift/latest/dg/r_STV_LOCKS.html)** - Amazon Redshift locks tables to prevent two users from updating the same table at the same time, STV_LOCKS can be used to view any current updates on tables in the database, need superuser to view\\n\\n```sql\\nselect \\n\\ttable_id, \\n\\tlast_update, \\n\\tlast_commit, \\n\\tlock_owner, \\n\\tlock_owner_pid, \\n\\tlock_status from stv_locks  \\norder by last_update asc;\\n```\\n\\n**[STL_TR_CONFLICT](https://docs.aws.amazon.com/redshift/latest/dg/r_STL_TR_CONFLICT.html)** - A transaction conflict occurs when two or more users are querying and modifying data rows from tables such that their transactions cannot be serialized. Every time a transaction conflict occurs, Amazon Redshift writes a log about the aborted transaction to the STL_TR_CONFLICT table. \\n\\n```sql\\nselect * from stl_tr_conflict \\nwhere table_id=<Table Id from STV_LOCKS>\\norder by xact_start_ts;\\n```\\n\\n**[SVV_TRANSACTIONS](https://docs.aws.amazon.com/redshift/latest/dg/r_SVV_TRANSACTIONS.html)** - Redshift uses this table to records information about transactions that currently hold locks on tables in the database\\n\\n```sql\\nselect * from svv_transactions;\\n```\\n\\n## Important Queries\\n\\n### Queries waiting in queue\\n\\nIf the query is running for more then expected the first thing you would like to do is figure out if the query actually executing or laying in the queue waiting for its turn.\\n\\nTo find out queries that are not truly \\"in flight\\" i.e waiting in the queue of blocked by some other query\\n\\n```sql\\nselect * from stv_recents \\nwhere status<>\'Done\' and pid not in (select pid from stv_inflight)\\n```\\n\\n### Long Running Queries\\n\\nIn case you are curious to know who else is delayed or running for long time, this query can help you find out list of all the queries running longer then 30 mints.\\n\\n```sql\\nselect \\n\\tpid, \\n\\ttrim(user_name) AS user_name, \\n\\tstarttime, \\n\\tquery, \\n\\tDATEDIFF(minutes, starttime, getdate()) as delay_in_mints, \\n\\tstatus\\nfrom stv_recents \\nwhere \\n\\tstatus=\'Running\' and  \\n\\tDATEDIFF(minutes, starttime, getdate()) > 30\\norder by starttime;\\n```\\n\\n### Blocking Queries\\n\\nTo find out the cause you must verify the locks this query can be used to find out what are the queries which have been granted the lock for the resources and what are the queries blocked by it or waiting for the same lock\\n\\n```sql\\nselect \\n\\tb.*, \\n\\tw.pid as blocked_pid, \\n\\tw.txn_owner as blocked_owner, \\n\\tDATEDIFF(minutes, b.txn_start, getdate()) as blocked_for_mints\\nfrom SVV_TRANSACTIONS b inner join SVV_TRANSACTIONS w \\n\\t\\ton b.txn_db = w.txn_db and b.relation = w.relation\\nwhere b.granted=\'t\' and w.granted = \'f\' \\n\\tand DATEDIFF(minutes, b.txn_start, getdate()) > 5\\norder by txn_start, b.pid;\\n```\\n\\n`granted = t`  means lock has been granted\\n\\n`granted = f`  means lock is pending\\n\\n### DeadLocked\\n\\nRedshift documentation recommends using **[STV_LOCKS](http://docs.aws.amazon.com/redshift/latest/dg/r_STV_LOCKS.html)** table to identify locks, this table works well until you hit a real deadlock, [PG_LOCKS](https://wiki.postgresql.org/wiki/Lock_Monitoring) could be the real life saving table that should be looked into.   \\n\\n```sql\\nselect \\n  current_time, \\n  c.relname, \\n  l.database, \\n  l.transaction, \\n  l.pid, \\n  a.usename, \\n  l.mode, \\n  l.granted\\nfrom pg_locks l \\njoin pg_catalog.pg_class c ON c.oid = l.relation\\njoin pg_catalog.pg_stat_activity a ON a.procpid = l.pid\\nwhere l.pid <> pg_backend_pid();\\n```\\n\\n## How to cancel a query?\\n\\n`cancel` can be used to Kill a query with the query pid and an optional message which will be returned to the issuer of the query and logged. PG_CANCEL_BACKEND is functionally equivalent to the CANCEL command.\\n\\n```sql\\ncancel <pid> \'Long-running query\';\\n```\\n\\n```sql\\nselect pg_cancel_backend(<pid>);\\n```\\n\\nIf the query that you canceled is associated with a transaction, use the ABORT or ROLLBACK. command to cancel the transaction and discard any changes made to the data:\\n\\n```sql\\nabort;\\n```\\n\\nPG_TERMINATE_BACKEND can be used to Terminates a session.\\n\\n```sql\\nselect pg_terminate_backend(<pid>);\\n```\\n\\nUnless you are signed on as a superuser, you can cancel only your own queries/session. A superuser can cancel all queries/session.\\n\\n## Bonus\\n\\nSome more Tables to for more informations\\n\\n**[SVL_QLOG](https://docs.aws.amazon.com/redshift/latest/dg/r_SVL_QLOG.html)** - Redshift also stores the past few days of queries in `svl_qlog` if you need to go back further\\n\\n```sql\\nselect userid, query, pid, starttime, endtime, elapsed, left(\\"substring\\", 50) as text from svl_qlog limit 10;\\n```\\n\\n**[STL_QUERYTEXT](https://docs.aws.amazon.com/redshift/latest/dg/r_STL_QUERYTEXT.html)**  All of the above tables only store the first 200 characters of each query. The full query is stored in chunks in `stl_querytext`. Join this table in by `query`, and sort by `query_id` and `sequence` to get each 200 character chunk in order\\n\\n```sql\\nselect query, starttime, text, \\"sequence\\" \\nfrom stl_query join stl_querytext using (query) \\norder by query,sequence \\nlimit 5;\\n```\\n\\nList of queries currently in-flight with user details \\n\\n```sql\\nselect \\n\\ta.userid, \\n\\tcast(u.usename as varchar(100)), \\n\\ta.query, \\n\\ta.label, \\n\\ta.pid, \\n\\ta.starttime, \\n\\tDATEDIFF(minutes, a.starttime, getdate()) as delay_in_mints, \\n\\tb.query as querytext\\nfrom stv_inflight a, stv_recents b, pg_user u\\nwhere a.pid = b.pid and a.userid = u.usesysid;\\n```"},{"id":"spell-correct-in-python-part-1","metadata":{"permalink":"/develbyte/blog/spell-correct-in-python-part-1","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2018-02-06-spell-correct-in-python-part-1.md","source":"@site/blog/2018-02-06-spell-correct-in-python-part-1.md","title":"Spell-correct in Python - Part 1","description":"One of the most common problem to face while solving a text processing use case is wrongly spelt words, swapping every wrongly spelt word with a mapping dictionary is a tedious solution. the major problem with this solution is the dictionary keeps growing and hard to manage.","date":"2018-02-06T00:00:00.000Z","tags":[{"inline":false,"label":"Python","permalink":"/develbyte/blog/tags/python","description":"Python programming language"},{"inline":false,"label":"Machine Learning","permalink":"/develbyte/blog/tags/machine-learning","description":"Machine learning concepts and techniques"},{"inline":false,"label":"Probability","permalink":"/develbyte/blog/tags/probability","description":"Probability theory and statistics"},{"inline":false,"label":"Bayes\' Theorem","permalink":"/develbyte/blog/tags/bayes-theorem","description":"Bayesian probability theory"},{"inline":false,"label":"Natural Language Processing","permalink":"/develbyte/blog/tags/nlp","description":"Natural language processing techniques"}],"readingTime":2.25,"hasTruncateMarker":false,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","page":{"permalink":"/develbyte/blog/authors/narendra"},"socials":{"github":"https://github.com/im-naren","linkedin":"https://www.linkedin.com/in/im-naren/","email":"mailto:naren.dubey@zoho.com"},"imageURL":"https://github.com/im-naren.png","key":"narendra"}],"frontMatter":{"slug":"spell-correct-in-python-part-1","title":"Spell-correct in Python - Part 1","authors":["narendra"],"tags":["python","machine-learning","probability","bayes-theorem","nlp"],"date":"2018-02-06T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Troubleshooting in Redshift","permalink":"/develbyte/blog/troubleshooting-in-redshift"},"nextItem":{"title":"Introduction To Pandas","permalink":"/develbyte/blog/introduction-to-pandas"}},"content":"One of the most common problem to face while solving a text processing use case is wrongly spelt words, swapping every wrongly spelt word with a mapping dictionary is a tedious solution. the major problem with this solution is the dictionary keeps growing and hard to manage.\\n\\nThere is a beautiful solution demonstrated by [Peter Norvig](http://norvig.com/) which solves the problem of wrong spelling for 70% of the cases. \\n\\n## Solution\\n\\nHere are 36 lines of code which solves the problem fairly easily and quickly. The basic idea here is to find the most probable word to replace the wrongly spelt word up to 2 edits.\\n\\n### spell_check.py\\n\\n```python\\nimport re\\nfrom collections import Counter\\n\\ndef words(text): return re.findall(r\'\\\\w+\', text.lower())\\n\\n# Note: You\'ll need to provide a text corpus file (big.txt) \\n# containing a large amount of text in your target language\\nWORDS = Counter(words(open(\'big.txt\').read()))\\n\\ndef P(word, N=sum(WORDS.values())): \\n    \\"Probability of `word`.\\"\\n    return WORDS[word] / N\\n\\ndef correction(word): \\n    \\"Most probable spelling correction for word.\\"\\n    return max(candidates(word), key=P)\\n\\ndef candidates(word): \\n    \\"Generate possible spelling corrections for word.\\"\\n    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\\n\\ndef known(words): \\n    \\"The subset of `words` that appear in the dictionary of WORDS.\\"\\n    return set(w for w in words if w in WORDS)\\n\\ndef edits1(word):\\n    \\"All edits that are one edit away from `word`.\\"\\n    letters    = \'abcdefghijklmnopqrstuvwxyz\'\\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\\n    deletes    = [L + R[1:]               for L, R in splits if R]\\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\\n    inserts    = [L + c + R               for L, R in splits for c in letters]\\n    return set(deletes + transposes + replaces + inserts)\\n\\ndef edits2(word): \\n    \\"All edits that are two edits away from `word`.\\"\\n    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\\n```\\n\\n### Output\\n\\n```python\\n>>> correction(\'speling\')\\n\'spelling\'\\n\\n>>> correction(\'korrectud\')\\n\'corrected\'\\n```\\n\\n## How does it works\\n\\nfirst of all, we compute the probability of occurrence all the word words in the preferred language in the example above we have chosen English as the language of preference and we will call if the language model.\\n\\nnext step is to figure all the possible word can be made after doing two edits (delete, insert, replaces, transposes) to a given word. the example above is limited to only two edits for now\\n\\nafter we have a distinct list of all the possible word the one with the highest probability score in the language model is accepted as the correct spelling."},{"id":"introduction-to-pandas","metadata":{"permalink":"/develbyte/blog/introduction-to-pandas","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2017-07-08-introduction-to-pandas.md","source":"@site/blog/2017-07-08-introduction-to-pandas.md","title":"Introduction To Pandas","description":"Pandas is one of the widely used Python libraries for working with data. it is built on libraries like Matplotlib and NumPy. Pandas is great for data manipulation, data analysis, and data visualization.","date":"2017-07-08T00:00:00.000Z","tags":[{"inline":false,"label":"Python","permalink":"/develbyte/blog/tags/python","description":"Python programming language"},{"inline":false,"label":"Machine Learning","permalink":"/develbyte/blog/tags/machine-learning","description":"Machine learning concepts and techniques"},{"inline":false,"label":"Data Science","permalink":"/develbyte/blog/tags/data-science","description":"Data science methodologies"},{"inline":false,"label":"Pandas","permalink":"/develbyte/blog/tags/pandas","description":"Pandas data manipulation library"}],"readingTime":6.48,"hasTruncateMarker":false,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","page":{"permalink":"/develbyte/blog/authors/narendra"},"socials":{"github":"https://github.com/im-naren","linkedin":"https://www.linkedin.com/in/im-naren/","email":"mailto:naren.dubey@zoho.com"},"imageURL":"https://github.com/im-naren.png","key":"narendra"}],"frontMatter":{"slug":"introduction-to-pandas","title":"Introduction To Pandas","authors":["narendra"],"tags":["python","machine-learning","data-science","pandas"],"date":"2017-07-08T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Spell-correct in Python - Part 1","permalink":"/develbyte/blog/spell-correct-in-python-part-1"},"nextItem":{"title":"Refreshing Probability","permalink":"/develbyte/blog/probability"}},"content":"**Pandas** is one of the widely used Python libraries for working with data. it is built on libraries like Matplotlib and NumPy. Pandas is great for data manipulation, data analysis, and data visualization.\\n\\nIn this tutorial we will see how pandas makes life really easy for a data analysis. Pandas can read and write data from and to CSV files or even databases easily.\\n\\n## Data Structures in Pandas\\n\\n### Series\\n\\nA series is a one-dimensional object, like an array, list or could be understood as a column in table. similar to the array or list index each element in a series is assigned with a labeled index. By default, each item is given an numerical index label from 0 to N, where N is the length of the Series minus one.\\n\\n#### How to create Series\\nThe basic method to create a Series is to call `.Series()`\\n\\n```python\\n# import pandas\\nimport pandas as pd\\n\\n# create a Series with an arbitrary list\\nX1 = pd.Series([7, \'develbyte\', 3.14, \'Happy Learnning\'])\\nX1\\n```\\n\\n    0                  7\\n    1          develbyte\\n    2               3.14\\n    3    Happy Learnning\\n    dtype: object\\n\\n**Note:-** when the Series contains elements of multiple different datatypes the dtype of the series will be the higher datatype\\n\\n`int32 > int64 > flot64 >.....>object`  \\n\\n```python\\nX2 = pd.Series([7, 5, 4, 3])\\nprint(X2)\\n\\nX3 = pd.Series([7, 5, 4., 3.])\\nprint(X3)\\n```\\n\\n    0    7\\n    1    5\\n    2    4\\n    3    3\\n    dtype: int64\\n    0    7.0\\n    1    5.0\\n    2    4.0\\n    3    3.0\\n    dtype: float64\\n\\n#### Creating a series with index\\nindex of the series elements can also be changes by simply passing a list of indexes, the list of elements and the list of indexes should be of same length or you will end up with error\\n\\n```python\\nX1 = pd.Series([7, \'develbyte\', 3.14, \'Happy Learnning\'],\\n              index=[\'A\', \'B\', \'C\', \'D\'])\\n\\nprint(X1)\\n```\\n\\n    A                  7\\n    B          develbyte\\n    C               3.14\\n    D    Happy Learnning\\n    dtype: object\\n\\n### DataFrame\\n\\nDataFrame is a two-dimensional labeled data structures with columns of same or different data types. Similar to tables in a database the DataFrame can hold multiple columns with multiple data types. You can also think of a DataFrame as a group of Series objects that share an index.\\n\\n#### How to import Data in Dataframe\\n\\n```python\\n# Importing the dataset\\ndataset = pd.read_csv(\'/data/introduction-to-panda-1.csv\')\\ndataset\\n```\\n\\nThis would display a table with Country, Age, Salary, and Purchased columns.\\n\\n#### How to inspect Data in Dataframe\\n\\nVery first information what we would like to know in a dataframe are:\\n- number of columns\\n- number of records\\n- attribute names\\n- datatype of each attribute    \\n\\nwe can get all these information by calling just one function `info()` it will give Concise summary of a DataFrame\\n\\n```python\\ndataset.info()\\n```\\n\\n```\\n<class \'pandas.core.frame.DataFrame\'>\\nRangeIndex: 10 entries, 0 to 9\\nData columns (total 4 columns):\\nCountry      10 non-null object\\nAge          9 non-null float64\\nSalary       9 non-null float64\\nPurchased    10 non-null object\\ndtypes: float64(2), object(2)\\nmemory usage: 392.0+ bytes\\n```\\n\\nwe can also use `dtypes` to get the datatypes of each attribute\\n\\n```python\\ndataset.dtypes\\n```\\n\\n    Country       object\\n    Age          float64\\n    Salary       float64\\n    Purchased     object\\n    dtype: object\\n\\nfor just getting the column names in a dataframe use `dataset.columns`\\n\\n```python\\n# columns.values gives the column names in the DataFrame\\ndataset.columns.values\\n```\\n\\n    array([\'Country\', \'Age\', \'Salary\', \'Purchased\'], dtype=object)\\n\\nsimilarly for index values\\n\\n```python\\n# index.values gives the list of row indices\\ndataset.index.values\\n```\\n\\n    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\\n\\nProbably the most useful function for inspecting a data set in a DataFrame is `describe()` it will return basic statistics about the dataset\'s numeric columns\\n\\n```python\\ndataset.describe()\\n```\\n\\nThis would show count, mean, std, min, 25%, 50%, 75%, and max for numeric columns.\\n\\nFor take a look at the actual data `head()` and `tail()` are the most useful function:\\n- head method shows first n rows from the DataFrame, default value of n is 5\\n- tail method shows last n rows from the DataFrame, default value of n is 5\\n\\n```python\\ndataset.head()\\ndataset.tail()\\n```\\n\\n## Select and Index in DataFrames\\n\\nThere are three main options in pandas, which allows us to access the data in DataFrame, these are based on index and location of the rows and column, these options could be confusing for beginners but its quite simple once understood.\\n\\nThe selection methods are:\\n\\n- Selecting data by row numbers called integer-location based indexing/selection (`.iloc`)\\n- Selecting data by label or by a conditional statement (`.loc`)\\n- Selecting in a hybrid approach(`.ix`)\\n\\n### Single selections using iloc and DataFrame\\n\\n#### Rows:\\n\\n```python\\nR1 = dataset.iloc[0] # first row of data frame - Note a Series data type output.\\nR2 = dataset.iloc[1] # second row of data frame\\nR3 = dataset.iloc[-1] # last row of data frame\\n```\\n\\n#### Columns:\\n\\n```python\\nC1 = dataset.iloc[:,0] # first column of data frame (first_name)\\nC2 = dataset.iloc[:,1] # second column of data frame (last_name)\\nC3 = dataset.iloc[:,-1] # last column of data frame (id)\\n```\\n\\n### Multiple row and column selections using `iloc` and `DataFrame`\\n\\n```python\\nMR1 = dataset.iloc[0:5] # first five rows of dataframe\\nMR2 = dataset.iloc[:, 0:2] # first two columns of data frame with all rows\\nMR3 = dataset.iloc[[0, 3, 6, 9], [0, 3]] # 1st, 4th, 7th, 9th row + 1st 3th 4th columns.\\nMR4 = dataset.iloc[0:5, 1:3] # first 5 rows and 4th, 5th columns of data frame.\\n```\\n\\n`.iloc` returns a Pandas Series when one only row or Column is selected\\n\\n`.iloc` returns a Pandas DataFrame when multiple rows or Columns are selected\\n\\n`.iloc` returns a Pandas Series when multiple rows are selected with only one column\\n\\nabove output could easily converted to Pandas `DataFrame` by passing a single-valued list as column index\\n\\n```python\\nprint(type(dataset.iloc[1:2, [3]]))\\n```\\n\\n### Selecting pandas data using `loc`:\\n\\nThe Pandas `loc` indexer can be used with DataFrames in two different scenarios:\\n\\na.) Selecting rows by label/index\\nb.) Selecting rows with a Boolean/conditional lookup\\n\\nThe `loc` indexer is used with the same syntax as `iloc: data.loc[<row selection>, <column selection>]`\\n\\nIndex can be in a DataFrame by using `set_index()` method\\n\\n```python\\ndataset.set_index(\\"Country\\", inplace=True)\\ndataset\\n```\\n\\nSelecting rows by index\\n\\n```python\\ndataset.loc[\'France\']\\n```\\n\\nSelecting rows by label/index\\n\\n```python\\ndataset.loc[[\'France\', \'Spain\'], [\'Age\', \'Salary\']]\\n```\\n\\nSelecting rows with a Boolean/conditional lookup\\n\\n```python\\ndataset.loc[dataset[\'Country\'] == \'France\', [\'Country\', \'Salary\']]\\n```\\n\\nSelections can be achieved outside of the main `.loc` for clarity\\nForm a separate variable with your selections:\\n\\n```python\\nidx = dataset[\'Country\'].apply(lambda x: x.lower() == \'france\')\\ndataset.loc[idx, [\'Country\', \'Salary\']]\\n```\\n\\n### Selecting pandas data using `ix`:\\n\\n- `ix[]` indexer is a hybrid of `.loc` and `.iloc`,\\n- `ix` is label based indexer, it behave just like `.loc`, it also supports integer based indexing like `.iloc`\\n- `ix` indexing works just the same as `.loc` when passed strings\\n\\n```python\\ndataset.ix[[\'Country\']] == dataset.loc[[\'Country\']]\\n```\\n\\n`ix` indexing works the same as `.iloc` when passed integers\\n\\n```python\\ndataset.reset_index(inplace=True)\\ndataset.ix[[2]] == dataset.iloc[[2]]\\n```\\n\\n## Add and Delete in DataFrame\\n\\n### Adding row in DataFrame\\n\\nNote:- General recommendation for adding a row is to use `.loc` to insert rows in DataFrame\\nIf you would use `.ix`, you might try to reference a numerically valued index with the index value and accidentally overwrite an existing row of your DataFrame.\\n\\n```python\\ndataset.loc[10] = [\'India\', 27, 65000, \'Yes\']\\ndataset.ix[11] = [\'India\', 26, 60000, \'Yes\']\\ndataset\\n```\\n\\n### Adding column in DataFrame\\n\\nColumns in DataFrame is basically a series,so adding a column in a DataFrame is as simple as assigning a new column to a DataFrame\\n\\n```python\\ncolumn = pd.Series(range(1,13), dtype=float)\\ncolumn\\n\\n#adding column in DataFrame\\ndataset[\'id\'] = column\\ndataset\\n```\\n\\n### Delete a column from DataFrame by column name\\n\\n```python\\ndf = dataset.drop(\'Purchased\', axis=1)\\ndf\\n```\\n\\n### Delete a column from DataFrame by row index\\n\\none thing to be noted here is if the in-place is set to be True the deleting happens the the existing dataframe\\nby default in-place is False, which creates new DataFrame with deleted rows\\n\\n```python\\ndataset.drop(dataset.index[2:7], inplace=True)\\ndataset\\n```"},{"id":"probability","metadata":{"permalink":"/develbyte/blog/probability","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2017-04-10-probability.md","source":"@site/blog/2017-04-10-probability.md","title":"Refreshing Probability","description":"Probability:","date":"2017-04-10T00:00:00.000Z","tags":[{"inline":false,"label":"Machine Learning","permalink":"/develbyte/blog/tags/machine-learning","description":"Machine learning concepts and techniques"},{"inline":false,"label":"Data Science","permalink":"/develbyte/blog/tags/data-science","description":"Data science methodologies"},{"inline":false,"label":"Probability","permalink":"/develbyte/blog/tags/probability","description":"Probability theory and statistics"},{"inline":false,"label":"Statistics","permalink":"/develbyte/blog/tags/statistics","description":"Statistical concepts and methods"}],"readingTime":5.18,"hasTruncateMarker":false,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","page":{"permalink":"/develbyte/blog/authors/narendra"},"socials":{"github":"https://github.com/im-naren","linkedin":"https://www.linkedin.com/in/im-naren/","email":"mailto:naren.dubey@zoho.com"},"imageURL":"https://github.com/im-naren.png","key":"narendra"}],"frontMatter":{"slug":"probability","title":"Refreshing Probability","authors":["narendra"],"tags":["machine-learning","data-science","probability","statistics"],"date":"2017-04-10T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Introduction To Pandas","permalink":"/develbyte/blog/introduction-to-pandas"},"nextItem":{"title":"Zookeeper Sessions and life cycle","permalink":"/develbyte/blog/zookeeper-sessions-and-life-cycle"}},"content":"## Probability:\\n\\nis defined as the measure of the likelihood that an event will occur. Measured by the ratio of the favorable event to the whole number of events possible. Probability is quantified as a number between 0 and 1 (where 0 indicates impossibility and 1 indicates certainty). The higher the probability of an event, the more certain that the event will occur.\\n\\n## Probability Key Terms:\\n\\n### Experiment:-\\nAn experiment in probability is a test to see what will happen in case you do something. A simple example is flipping a coin. When you flip a coin, you are performing an experiment to see what side of the coin you\'ll end up with.\\n\\n### Random Experiment:-\\nAn experiment whose outcome has to be among a set of events that are completely known but whose exact outcome is unknown is a random experiment (e.g. Throwing of a dice, tossing of a coin). Most questions on probability are based on random experiments.\\n\\n### Outcome:-\\nAn outcome in probability refers to a single (one) result of an experiment. In the example of an experiment above, one outcome would be heads and the other would be tails.\\n\\n### Event:-\\nAn event in probability is the set of a group of different outcomes of an experiment. Suppose you flip a coin multiple times, an example of an event would the getting a certain number of heads.\\n\\n### Non-event:-\\nThe outcome that is opposite the desired outcome is the non-event. Note that if the event occurs, the non-event does not occur and vice-verse.\\n\\n### Sample space:-\\nThe collection of all possible results is called the sample space of an probabilistic experiment.\\n\\n### Independent Event:-\\nTwo events are said to be independent, if the outcome of one of the events doesn\'t affect the outcome of another. For example, if we throw two dice, the probability of getting a 6 on the second die is the same, no matter what we get with the first one- it\'s still 1/6.\\n\\n### Dependent Event:-\\nWhen the probability of one event depends on another, the events are called dependent event. For example, if we have a bag containing 2 red and 2 blue balls. If we pick 2 balls out of the bag, the probability that the second is blue depends upon what the color of the first ball picked was. If the first ball was blue, there will be 1 blue and 2 red balls in the bag when we pick the second ball. So the probability of getting a blue is 1/3. However, if the first ball was red, there will be 1 red and 2 blue balls left so the probability the second ball is blue is 2/3.\\n\\n### Mutually exclusive events:-\\nA set of events is mutually exclusive when the occurrence of any one of them means that the other events cannot occur. For a given sample space, its either one or the other but not both. As a consequence, mutually exclusive events have their probability defined as follows:\\n\\n\\t\\t`P(A) + P(B) = 1`\\n\\nAn example of mutually exclusive events are the outcomes of a fair coin flip. When you flip a fair coin, you either get a head or a tail but not both, we can prove that these events are mutually exclusive by adding their probabilities\\n\\n\\t\\tP(head) + P(tail) = 1/2 + 1/2 =  1\\n\\nIf A and B are Mutually Exclusive Events:\\n\\n\\t\\t`P(A intersection B) = 0`\\n\\t\\t`P(A + B) = 1`\\n\\t\\t`P(A union B)\' = 0`\\n\\t\\t`P(B|A) = 0`\\n\\n### Equally Likely Events:-\\nIf two events have the same probability or chance of occurrence they are called equally likely events. (In a throw of a dice, the chance of 1 showing on the dice is equal to 2 is equal to 3 is equal to 4 is equal to 5 is equal to 6 appearing on the dice.)\\n\\n## Conditional probability:\\n\\nIt is the probability of the occurrence of an event `A` given that the event `B` has already occurred. This is denoted by `P(A/B)`. (E.g. The probability that in two throws of a dices we get a total of 7 or more given that in the first throw of the dices the number 5 had occurred.)\\n\\nConditional probability is denoted by the following: `P(B|A)`\\n\\nThe probability that `B` occurs given that `A` has already occurred\\n\\nThe above is mathematically defined as:\\n\\n\\t\\t`P(B|A) = P(A intersection B)/P(A)`\\n\\n## The concept of Odds For and Odds Against:\\n\\nSometimes, probability is also viewed in terms of odds for and odds against an event.\\n\\nOdds in favor of event E is defined as: `P(E)/P(E)\'`\\nOdds against as event is defined as: `P(E)/P(E)\'`\\n\\n## Notation of Probability\\n\\nsample space : `S={A, B}`, where A and  B are independent events. example `S={head, tail}`\\n\\nThe probability of an event `A` to occur is written as `P(A)`, `p(A)` or `Pr(A)`\\n\\nThe probability of an event `A` to not occur is written as `-A`, `~A`, `A\'`;\\n\\nIts probability is given by\\n\\n\\t\\tP(not A) = 1 \u2212 P(A) = P(A)\'\\n\\n## Set Theory in Probability:\\n\\nThe entire sample space of S is given by:\\n\\n\\t\\t`S = {A, B, C}`\\n\\nRemember the following from set theory:\\n\\n\\t\\t`C = (A union B)\'`\\n\\n\\t\\t`(A union B) = A + B - (A intersection B)`\\n\\n## Rules of Probability:\\n\\n\\tFor `S = {A, B, C}`\\n\\t\\t`P(S) = P (A union B union C) = 1`\\n\\n- **Multiplication Rule** `(A intersection B)`\\n\\n\\tIf A and B are dependent events, the probability of this event happening can be calculated as shown below:\\n\\n\\t\\t`P(A intersection B) = P(A*B) = P(A.B) = P(A union B) - (P(A) + P(B))`\\n\\n\\tIf A and B are independent events, the probability of this event happening can be calculated as shown below:\\n\\n\\t\\t`P(A intersection B) = P(A*B) = P(A.B) = P(A) * P(B)`\\n\\n\\tConditional probability for two independent events can be redefined using the relationship above to become:\\n\\n\\t\\t`P(B|A) = P(A intersection B)/P(A)`\\n\\n\\t\\t`P(B|A) = P(A) * P(B)/ P(A)`\\n\\n\\t\\t`P(B|A) = P(B)`\\n\\n- **Additive Rule** (A union B):\\n\\n\\t\\t`P(A + B) = P(A union B)`\\n\\n\\t\\t`P(A + B) = P(A) + P(B) - P(A intersection B)`"},{"id":"zookeeper-sessions-and-life-cycle","metadata":{"permalink":"/develbyte/blog/zookeeper-sessions-and-life-cycle","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2016-11-04-zookeeper-sessions-and-life-cycle.md","source":"@site/blog/2016-11-04-zookeeper-sessions-and-life-cycle.md","title":"Zookeeper Sessions and life cycle","description":"Session and request order handling:","date":"2016-11-04T00:00:00.000Z","tags":[{"inline":false,"label":"Zookeeper","permalink":"/develbyte/blog/tags/zookeeper","description":"Apache Zookeeper coordination service"},{"inline":false,"label":"Distributed Computing","permalink":"/develbyte/blog/tags/distributed-computing","description":"Distributed computing technologies"},{"inline":false,"label":"Kafka","permalink":"/develbyte/blog/tags/kafka","description":"Apache Kafka messaging system"},{"inline":false,"label":"Coordination","permalink":"/develbyte/blog/tags/coordination","description":"System coordination and synchronization"}],"readingTime":1.65,"hasTruncateMarker":false,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","page":{"permalink":"/develbyte/blog/authors/narendra"},"socials":{"github":"https://github.com/im-naren","linkedin":"https://www.linkedin.com/in/im-naren/","email":"mailto:naren.dubey@zoho.com"},"imageURL":"https://github.com/im-naren.png","key":"narendra"}],"frontMatter":{"slug":"zookeeper-sessions-and-life-cycle","title":"Zookeeper Sessions and life cycle","authors":["narendra"],"tags":["zookeeper","distributed-computing","kafka","coordination"],"date":"2016-11-04T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Refreshing Probability","permalink":"/develbyte/blog/probability"},"nextItem":{"title":"Zookeeper Namespace And Operations","permalink":"/develbyte/blog/zookeeper-namespace-and-operations"}},"content":"## Session and request order handling:\\n\\nSessions is very important and quite critical for the operation of ZooKeeper. All operations a client submits to ZooKeeper are associated to a session. When a session ends for any reason, the ephemeral nodes created during that session disappear.\\n\\nThe client initially connects to any server in the ensemble, and only to a single server. It uses a TCP connection to communicate with the server, but the session may be moved to a different server if the client has not heard from its current server for some time. Moving a session to a different server is handled transparently by the ZooKeeper client library.\\n\\nSessions offer order guarantees, which means that requests in a session are executed in FIFO (first in, first out) order. Typically, a client has only a single session open, so its requests are all executed in FIFO order. When a client creates a ZooKeeper handle using a specific language binding, it establishes a session with the service. If a client has multiple concurrent sessions, FIFO ordering is not necessarily preserved across the sessions. Consecutive sessions of the same client, even if they don\'t overlap in time, also do not necessarily preserve FIFO order.\\n\\n### Here is how it can happen in this case:\\n- Client establishes a session and makes two consecutive asynchronous calls to `create /tasks` and `create /workers`.\\n- First session expires.\\n- Client establishes another session and makes an asynchronous call to `create /assign`.\\nIn this sequence of calls, it is possible that only `/tasks` and `/assign` have been created, which preserves FIFO ordering for the first session but violates it across sessions.\\n\\n## States and the Lifetime of a Session\\n\\nThe lifetime of a session is the period between its creation and its end, whether it is closed gracefully or expires because of a timeout. The possible states of a session are : *CONNECTING*, *CONNECTED*, *CLOSED*, and *NOT_CONNECTED*.\\n\\n![states-and-the-Lifetime-of-a-Session](/img/states-and-the-Lifetime-of-a-Session.png)"},{"id":"zookeeper-namespace-and-operations","metadata":{"permalink":"/develbyte/blog/zookeeper-namespace-and-operations","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2016-10-27-zookeeper-namespace-and-operations.md","source":"@site/blog/2016-10-27-zookeeper-namespace-and-operations.md","title":"Zookeeper Namespace And Operations","description":"Zookeper data Model:","date":"2016-10-27T00:00:00.000Z","tags":[{"inline":false,"label":"Zookeeper","permalink":"/develbyte/blog/tags/zookeeper","description":"Apache Zookeeper coordination service"},{"inline":false,"label":"Distributed Computing","permalink":"/develbyte/blog/tags/distributed-computing","description":"Distributed computing technologies"},{"inline":false,"label":"Kafka","permalink":"/develbyte/blog/tags/kafka","description":"Apache Kafka messaging system"},{"inline":false,"label":"Coordination","permalink":"/develbyte/blog/tags/coordination","description":"System coordination and synchronization"}],"readingTime":4.63,"hasTruncateMarker":false,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","page":{"permalink":"/develbyte/blog/authors/narendra"},"socials":{"github":"https://github.com/im-naren","linkedin":"https://www.linkedin.com/in/im-naren/","email":"mailto:naren.dubey@zoho.com"},"imageURL":"https://github.com/im-naren.png","key":"narendra"}],"frontMatter":{"slug":"zookeeper-namespace-and-operations","title":"Zookeeper Namespace And Operations","authors":["narendra"],"tags":["zookeeper","distributed-computing","kafka","coordination"],"date":"2016-10-27T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Zookeeper Sessions and life cycle","permalink":"/develbyte/blog/zookeeper-sessions-and-life-cycle"},"nextItem":{"title":"Zookeeper Introduction to Zookeeper","permalink":"/develbyte/blog/zookeeper-introduction-to-zookeeper"}},"content":"## Zookeper data Model:\\n\\nZooKeeper has a hierarchal name space(as shown below), much like a distributed file system. The only difference is that each node in the namespace can have data associated with it as well as children. It is like having a file system that allows a file to also be a directory.\\n\\n![zookeeper-data-model](/img/zookeeper-data-model.png)\\n\\nThe root node contains four more nodes, and three of those nodes have nodes under them. The leaf nodes are the data.\\n\\n## Znode:\\n\\nEvery node in a ZooKeeper tree is referred to as a znode.Znodes maintain a stat structure that includes version numbers for data changes, acl changes. The stat structure also has timestamps. The version number, together with the timestamp, allows ZooKeeper to validate the cache and to coordinate updates. Each time a znode\'s data changes, the version number increases.\\n\\nZnodes may or may not contain data. If a znode contains any data, the data is stored as a  byte  array.  The  exact  format  of  the  byte  array  is  specific  to  each  application,  and ZooKeeper does not directly provide support to parse it. The absence of data often conveys important information about a znode. For Example :- in a typical master-worker application the absence of a master znode means that no master is currently elected.\\n\\n### The ZooKeeper API exposes the following operations:\\n\\n- `create /path data` - Creates a znode named with /path  and containing data\\n- `delete /path` - Deletes the znode  /path\\n- `exists /path` - Checks whether /path exists\\n- `setData /path data` - Sets the data of znode /path to  data\\n- `getData /path` - Returns the data in /path\\n- `getChildren /path` - Returns the list of children under /path\\n\\nOne important note is that ZooKeeper does not allow partial writes or reads of the znode data. When setting the data of a znode or reading it, the content of the znode is replaced or read entirely.\\n\\n## Different Modes of Znode:\\n\\nZnodes can be created with four different modes: persistent, ephemeral, presistent_sequential and ephemeral_sequential\\n\\n### Persistent and ephemeral znodes\\n\\nA znode can be either persistent or ephemeral. A persistent znode /path can be deleted only through a call to delete. An ephemeral znode, in contrast, is deleted if the client that created it crashes or simply closes its connection to ZooKeeper.\\n\\nPersistent znodes are useful when the znode stores some data on behalf of an application and this data needs to be preserved even after its creator is no longer part of the system.\\n\\nEphemeral znodes convey information about some aspect of the application that must exist only while the session of its creator is valid.\\n\\n### Sequential znodes:\\n\\nA sequential znode is assigned a unique, monotonically increasing integer. This sequence number is appended to the path used to create the znode. For example, if a client creates a sequential znode with the path /tasks/ task-, ZooKeeper assigns a sequence number, say 1, and appends it to the path. The path of the znode becomes /tasks/task-1. Sequential znodes provide an easy way to create znodes with unique names. They also provide a way to easily see the creation order of znodes.\\n\\n## Version:\\n\\nEach znode has a version number associated with it that is incremented every time its data changes. A couple of operations in the API can be executed conditionally: setData and delete. Both calls take a version as an input parameter, and the operation succeeds only if the version passed by the client matches the current version on the server. The use of versions is important when multiple ZooKeeper clients might be trying to perform operations over the same znode.\\n\\n## Watch:\\n\\nA watch is a one-shot operation, which means that it triggers one notification for any changes to znodes. Registering to receive a notification for a given znode consists of setting a watch. To receive multiple notifications over time, the client must set a new watch upon receiving each notification.\\n\\n## Data Access:\\n\\nThe data stored at each znode in a namespace is read and written atomically. Reads get all the data bytes associated with a znode and a write replaces all the data. Each node has an Access Control List (ACL) that restricts who can do what.\\n\\nZooKeeper was not designed to be a general database or large object store. Instead, it manages coordination data. This data can come in the form of configuration, status information, rendezvous, etc.\\n\\n## Semantics of Watches :\\n\\nWe can set watches with the three calls that read the state of ZooKeeper: `exists`, `getData`, and `getChildren`.The following list details the events that a watch can trigger and the calls that enable them:\\n\\n- **Created event**: Enabled with a call to exists.\\n- **Deleted event**: Enabled with a call to exists, `getData`, and `getChildren`.\\n- **Changed event**: Enabled with a call to exists and `getData`.\\n- **Child event**: Enabled with a call to `getChildren`.\\n\\n## Remove Watches:\\n\\nWe can remove the watches registered on a znode with a call to `removeWatches`. Also, a ZooKeeper client can remove watches locally even if there is no server connection by setting the local flag to true. The following list details the events which will be triggered after the successful watch removal.\\n\\n- **Child Remove event**: Watcher which was added with a call to `getChildren`.\\n- **Data Remove event**: Watcher which was added with a call to exists or `getData`.\\n\\n## ACL Permissions :\\n\\nZooKeeper supports the following permissions:\\n\\n- `CREATE`: you can create a child node\\n- `READ`: you can get data from a node and list its children.\\n- `WRITE`: you can set data for a node\\n- `DELETE`: you can delete a child node\\n- `ADMIN`: you can set permissions"},{"id":"zookeeper-introduction-to-zookeeper","metadata":{"permalink":"/develbyte/blog/zookeeper-introduction-to-zookeeper","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2016-10-20-zookeeper-introduction-to-zookeeper.md","source":"@site/blog/2016-10-20-zookeeper-introduction-to-zookeeper.md","title":"Zookeeper Introduction to Zookeeper","description":"Zookeper is an open-source, centralised co-ordination service which is used to co-ordinate the services and manage the configurations of applications accross a large number of hosts over a distributed environment.","date":"2016-10-20T00:00:00.000Z","tags":[{"inline":false,"label":"Zookeeper","permalink":"/develbyte/blog/tags/zookeeper","description":"Apache Zookeeper coordination service"},{"inline":false,"label":"Distributed Computing","permalink":"/develbyte/blog/tags/distributed-computing","description":"Distributed computing technologies"},{"inline":false,"label":"Kafka","permalink":"/develbyte/blog/tags/kafka","description":"Apache Kafka messaging system"},{"inline":false,"label":"Coordination","permalink":"/develbyte/blog/tags/coordination","description":"System coordination and synchronization"}],"readingTime":2.17,"hasTruncateMarker":false,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","page":{"permalink":"/develbyte/blog/authors/narendra"},"socials":{"github":"https://github.com/im-naren","linkedin":"https://www.linkedin.com/in/im-naren/","email":"mailto:naren.dubey@zoho.com"},"imageURL":"https://github.com/im-naren.png","key":"narendra"}],"frontMatter":{"slug":"zookeeper-introduction-to-zookeeper","title":"Zookeeper Introduction to Zookeeper","authors":["narendra"],"tags":["zookeeper","distributed-computing","kafka","coordination"],"date":"2016-10-20T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Zookeeper Namespace And Operations","permalink":"/develbyte/blog/zookeeper-namespace-and-operations"},"nextItem":{"title":"Message Queue","permalink":"/develbyte/blog/message-queue"}},"content":"**Zookeper** is an open-source, centralised co-ordination service which is used to co-ordinate the services and manage the configurations of applications accross a large number of hosts over a distributed environment.\\n\\nCo-ordinating between the services in a distributed application is a complex process. ZooKeeper was designed to be a robust service that enables application developers to focus mainly on their  application logic rather than coordination. It exposes a simple API, similar to filesystem API, that allows developers to implement common co\u2010ordination tasks, such as electing a master server,managing group membership, and managing metadata.\\n\\nZookeper is open-sorced to Apache by Yahoo. Apache Zookeper have became standard for organising the services in Hadoop, kafka and other distributed frameworks.\\n\\n## What is a distributed application?\\n\\nDistributed applications are the type of application comprised of multiple software components running independently and concurrently across multiple physical machines. Distributed applications are required to do resource intensive computions which is difficult or impossible to do in a non-distributed environment. The group is machines running the distributed applications are togethere called as cluster and each machine individually called as Nodes. Distributed applications provides scalability and performance along with other benifits, But it can also easly lead to various complexities like Race Condition, DeadLock, Inconsistency, This is where Zookeeper comes to rescue. Zookeper helps to create co-ordianation between the tasks and maintain shared data with robust syncronisation technique.\\n\\nLet\'s look at some examples where ZooKeeper has been useful to get a better sense of where it is applicable:\\n\\n## 1. Apache HBase\\nHBase is a data store typically used alongside Hadoop. In HBase, ZooKeeper is used to  elect  a  cluster  master,  to  keep  track  of  available  servers,  and  to  keep  cluster metadata.\\n\\n## 2. Apache Kafka\\nKafka is a pub-sub messaging system. It uses ZooKeeper to detect crashes, to implement topic discovery, and to maintain production and consumption state for topics.\\n\\n## 3. Apache Solr\\nSolr is an enterprise search platform. In its distributed form, called SolrCloud, it uses ZooKeeper to store metadata about the cluster and coordinate the updates to this metadata.\\n\\n## 4. Yahoo! Fetching Service\\nPart of a crawler implementation, the Fetching Service  fetches web pages efficiently by  caching content while making sure that web server policies, such as those in robots.txt files, are preserved. This service uses ZooKeeper for tasks such as master election, crash detection, and metadata storage.\\n\\n## 5. Facebook Messages\\nThis is a Facebook application that integrates communication channels: email, SMS, Facebook Chat, and the existing Facebook Inbox. It uses ZooKeeper as a controller for implementing sharding and failover, and also for service discovery."},{"id":"message-queue","metadata":{"permalink":"/develbyte/blog/message-queue","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2016-09-20-message-queue.md","source":"@site/blog/2016-09-20-message-queue.md","title":"Message Queue","description":"What is Message Passing?","date":"2016-09-20T00:00:00.000Z","tags":[{"inline":false,"label":"Big Data","permalink":"/develbyte/blog/tags/big-data","description":"Big data technologies and concepts"},{"inline":false,"label":"Kafka","permalink":"/develbyte/blog/tags/kafka","description":"Apache Kafka messaging system"},{"inline":false,"label":"Message Queue","permalink":"/develbyte/blog/tags/message-queue","description":"Message queuing systems"},{"inline":false,"label":"Distributed Systems","permalink":"/develbyte/blog/tags/distributed-systems","description":"Distributed computing concepts"}],"readingTime":1.64,"hasTruncateMarker":false,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","page":{"permalink":"/develbyte/blog/authors/narendra"},"socials":{"github":"https://github.com/im-naren","linkedin":"https://www.linkedin.com/in/im-naren/","email":"mailto:naren.dubey@zoho.com"},"imageURL":"https://github.com/im-naren.png","key":"narendra"}],"frontMatter":{"slug":"message-queue","title":"Message Queue","authors":["narendra"],"tags":["big-data","kafka","message-queue","distributed-systems"],"date":"2016-09-20T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Zookeeper Introduction to Zookeeper","permalink":"/develbyte/blog/zookeeper-introduction-to-zookeeper"},"nextItem":{"title":"Normalisation","permalink":"/develbyte/blog/normalisation"}},"content":"## What is Message Passing?\\n\\nMessage passing is a technique to enable inter-process communication (IPC), or for inter-thread communication within the same process communication between two distributed or non-distributed parallel processes in synchronous or asynchronous mode, The communications are completed by the sending of messages (functions, signals and data packets) to recipients.\\n\\nMost widely used messaging Patterns are:\\n- request-response\\n- Messaging-Queue\\n- Publisher-Subscriber\\n- RPC(remote-procedure-call)\\n- push-pull\\n\\n## Request Response:\\n\\nThe program that plays the role of servicing requests is called server. Correspondingly, the program that sends requests to a server is called client. We use server or client to refer to the role played by a program. A program can act as both server and client at the same time. this is most widely used in world wide web. we have both synchronuous and asynchronous versions of this.\\n\\n![client-server](/img/client-server-1.png)\\n\\n## Message Queue:\\n\\nMessage queues provide and asynchronous Point-to-point communications protocol, which means the sender puts messages in a queue and continue its processing without receiving an immediate response. the receiver can reach out to the messaging queue for receiving the messages. Messages placed onto the queue are stored until the recipient retrieves them. Message queues have implicit or explicit limits on the size of data that may be transmitted in a single message and the number of messages that may remain outstanding on the queue.\\n\\n![message-queue-model](/img/message-queue-model.png)\\n\\n## Publisher-Subscriber:\\n\\nPublish\u2013subscribe is a sibling of the message queue paradigm, this paradigm the sender of the message is called Publisher, who send the message to a topic without the knowledge of who are the specific receivers, called subscribers, similarly the subscribers receives the messages only of there interest, the messages gets filtered based on the topic and content. the only subscriber interested the particular topic or attribute, the matching constraints defined by the subscriber.\\n\\n![pub-sub-model](/img/pub-sub-model.png)"},{"id":"normalisation","metadata":{"permalink":"/develbyte/blog/normalisation","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2015-09-14-normalisation.md","source":"@site/blog/2015-09-14-normalisation.md","title":"Normalisation","description":"Normalisation is the process of eliminating the redundancy, minimising the use of null values and prevention of the loss of information by establishing relations and ensuring data integrity.","date":"2015-09-14T00:00:00.000Z","tags":[{"inline":false,"label":"SQL","permalink":"/develbyte/blog/tags/sql","description":"SQL queries and database operations"},{"inline":false,"label":"Database","permalink":"/develbyte/blog/tags/database","description":"Database design and management"},{"inline":false,"label":"Normalization","permalink":"/develbyte/blog/tags/normalization","description":"Database normalization concepts"},{"inline":false,"label":"Design","permalink":"/develbyte/blog/tags/design","description":"Design patterns and principles"}],"readingTime":3.55,"hasTruncateMarker":false,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","page":{"permalink":"/develbyte/blog/authors/narendra"},"socials":{"github":"https://github.com/im-naren","linkedin":"https://www.linkedin.com/in/im-naren/","email":"mailto:naren.dubey@zoho.com"},"imageURL":"https://github.com/im-naren.png","key":"narendra"}],"frontMatter":{"slug":"normalisation","title":"Normalisation","authors":["narendra"],"tags":["sql","database","normalization","design"],"date":"2015-09-14T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Message Queue","permalink":"/develbyte/blog/message-queue"},"nextItem":{"title":"How to sort a HashMap on Values","permalink":"/develbyte/blog/how-to-sort-a-hashmap-on-values"}},"content":"Normalisation is the process of eliminating the redundancy, minimising the use of null values and prevention of the loss of information by establishing relations and ensuring data integrity.\\n\\nData should only be stored once and avoid storing data that can be calculated from other data already held in the database. During the process of normalisation redundancy must be removed, but not at the expense of breaking data integrity rules.\\n\\nThe removal of redundancy helps to prevent insertion, deletion, and update errors, since the data is only available in one attribute of one table in the database.\\n\\nIf redundancy exists in the database then problems can arise when the database is in normal operation:\\n\\n- When data is inserted the data must be duplicated correctly in all places where there is redundancy. For instance, if two tables exist for in a database, and both tables contain the employee name, then creating a new employee entry requires that both tables be updated with the employee name.\\n- When data is modified in the database, if the data being changed has redundancy, then all versions of the redundant data must be updated simultaneously. So in the employee example a change to the employee name must happen in both tables simultaneously.\\n\\n## Aims of Normalisation\\n\\n- Normalisation ensures that the database is structured in the best possible way.\\n- To achieve control over data redundancy. There should be no unnecessary duplication of data in different tables.\\n- To ensure data consistency. Where duplication is necessary the data is the same.\\n- To ensure tables have a flexible structure. E.g. number of classes taken or books borrowed should not be limited.\\n- To allow data in different tables can be used in complex queries.\\n\\n## Stages of Normalisation\\n\\n- First Normal Form 1NF\\n- Second Normal Form 2NF\\n- Third Normal Form 3NF\\n- Boyce-Codd Normal Form BCNF\\n\\n## First Normal Form: 1NF\\n\\nA table is in its first normal form if it contains no repeating attributes or groups of attributes. To convert data for unnormalised form to 1NF, simply convert any repeated attributes into part of the candidate key.\\n\\n![un-normalise](/img/un-normalise.png)\\n\\n![first-degree-normalisation](/img/first-degree-normalisation.png)\\n\\n- A relation is in 1NF if it contains no repeating groups\\n- To convert an unnormalised relation to 1NF either\\n- Flatten the table and change the primary key, or\\n- Decompose the relation into smaller relations, one for the repeating groups and one for the non-repeating groups.\\n- Remember to put the primary key from the original relation into both new relations.\\n- This option is liable to give the best results.\\n\\n## Second Normal Form: 2NF\\n\\nA table is in the second normal form if it\'s in the first normal form AND no column that is not part of the primary key is dependant only a portion of the primary key.\\n\\nThe concept of functional dependency in central to normalisation and, in particular, strongly related to 2NF\\n\\n![second-degree-normalisation](/img/second-degree-normalisation.png)\\n\\n- A relation is in 2NF if it contains no repeating groups and no partial key functional dependencies\\n- Rule: A relation in 1NF with a single key field must be in 2NF\\n- To convert a relation with partial functional dependencies to 2NF. create a set of new relations:\\n- One relation for the attributes that are fully dependent upon the key.\\n- One relation for each part of the key that has partially dependent attributes\\n\\n## Third Normal Form: 3NF\\n\\nA table is in the third normal form if it is the second normal form and there are no non-key columns dependant on other non-key columns that could not act as the primary key.\\n\\n![third-degree-normalisation](/img/third-degree-normalisation.png)\\n\\n- A relation is in 3NF if it contains no repeating groups, no partial functional dependencies, and no transitive functional dependencies\\n- To convert a relation with transitive functional dependencies to 3NF, remove the attributes involved in the transitive dependency and put them in a new relation\\n- Rule: A relation in 2NF with only one non-key attribute must be in 3NF\\n- In a normalised relation a non-key field must provide a fact about the key, the whole key and nothing but the key.\\n- Relations in 3NF are sufficient for most practical database design problems. However, 3NF does not guarantee that all anomalies have been removed."},{"id":"how-to-sort-a-hashmap-on-values","metadata":{"permalink":"/develbyte/blog/how-to-sort-a-hashmap-on-values","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2015-05-25-how-to-sort-a-hashmap-on-values.md","source":"@site/blog/2015-05-25-how-to-sort-a-hashmap-on-values.md","title":"How to sort a HashMap on Values","description":"HashMap doesn\'t preserve any order by default. If order is required we need to sort it explicitly according to the requirement.","date":"2015-05-25T00:00:00.000Z","tags":[{"inline":false,"label":"Java","permalink":"/develbyte/blog/tags/java","description":"Java programming language"},{"inline":false,"label":"Coding","permalink":"/develbyte/blog/tags/coding","description":"Programming and coding practices"},{"inline":false,"label":"Data Structures","permalink":"/develbyte/blog/tags/data-structure","description":"Data structures and algorithms"},{"inline":false,"label":"Algorithms","permalink":"/develbyte/blog/tags/algorithms","description":"Algorithm design and analysis"}],"readingTime":1.11,"hasTruncateMarker":false,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","page":{"permalink":"/develbyte/blog/authors/narendra"},"socials":{"github":"https://github.com/im-naren","linkedin":"https://www.linkedin.com/in/im-naren/","email":"mailto:naren.dubey@zoho.com"},"imageURL":"https://github.com/im-naren.png","key":"narendra"}],"frontMatter":{"slug":"how-to-sort-a-hashmap-on-values","title":"How to sort a HashMap on Values","authors":["narendra"],"tags":["java","coding","data-structure","algorithms"],"date":"2015-05-25T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Normalisation","permalink":"/develbyte/blog/normalisation"},"nextItem":{"title":"How to Implement Singly Linked List in Java","permalink":"/develbyte/blog/how-to-implement-singly-linked-list-in-java"}},"content":"HashMap doesn\'t preserve any order by default. If order is required we need to sort it explicitly according to the requirement.\\n\\nIn this article I have tried to explain how to sort a HasMap based on values.\\n\\n## HasMap sorting by Value\\n\\nin this example I have used TreeMap to sort the HashMap. unlike a HashMap, a TreeMap guarantees that its elements will be sorted in ascending key order.\\n\\n### SortHashMap.java\\n\\n```java\\nimport java.util.Comparator;\\nimport java.util.HashMap;\\nimport java.util.Map;\\nimport java.util.TreeMap;\\n\\npublic class SortHashMap {\\n\\n    public static void main(String[] args) {\\n\\n        HashMap<String,Integer> map = new HashMap<String,Integer>();\\n        ValueComparator vcmp =  new ValueComparator(map);\\n        TreeMap<String,Integer> sorted_map = new TreeMap<String,Integer>(vcmp);\\n\\n        map.put(\\"naren\\",96);\\n        map.put(\\"ram\\",97);\\n        map.put(\\"manish\\",970);\\n        map.put(\\"kumar\\",97);\\n        map.put(\\"shruti\\",97);\\n        map.put(\\"rohit\\",760);\\n        map.put(\\"vatsal\\",444);\\n\\n        System.out.println(\\"unsorted map: \\"+map);\\n\\n        sorted_map.putAll(map);\\n\\n        System.out.println(\\"results: \\"+sorted_map);\\n    }\\n}\\n\\nclass ValueComparator implements Comparator<String> {\\n\\n    Map<String, Integer> base;\\n    public ValueComparator(Map<String, Integer> base) {\\n        this.base = base;\\n    }\\n\\n    // Note: this comparator imposes orderings that are inconsistent with equals.    \\n    public int compare(String a, String b) {\\n        if (base.get(a) >= base.get(b)) {\\n            return -1;\\n        } else {\\n            return 1;\\n        } // returning 0 would merge keys\\n    }\\n}\\n```\\n\\n### Output\\n\\n```java\\nunsorted map: {manish=970, kumar=97, ram=97, rohit=760, shruti=97, vatsal=444, naren=96}\\nresults: {manish=970, rohit=760, vatsal=444, shruti=97, ram=97, kumar=97, naren=96}\\n```"},{"id":"how-to-implement-singly-linked-list-in-java","metadata":{"permalink":"/develbyte/blog/how-to-implement-singly-linked-list-in-java","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2015-05-15-how-to-implement-singly-linked-list-in-java.md","source":"@site/blog/2015-05-15-how-to-implement-singly-linked-list-in-java.md","title":"How to Implement Singly Linked List in Java","description":"A Linked List is a dynamic data structure. The number of nodes in a list is not fixed and can grow and shrink on demand. Any application which has to deal with an unknown number of objects will need to use a linked list.","date":"2015-05-15T00:00:00.000Z","tags":[{"inline":false,"label":"Java","permalink":"/develbyte/blog/tags/java","description":"Java programming language"},{"inline":false,"label":"Coding","permalink":"/develbyte/blog/tags/coding","description":"Programming and coding practices"},{"inline":false,"label":"Data Structures","permalink":"/develbyte/blog/tags/data-structure","description":"Data structures and algorithms"},{"inline":false,"label":"Algorithms","permalink":"/develbyte/blog/tags/algorithms","description":"Algorithm design and analysis"}],"readingTime":2.31,"hasTruncateMarker":false,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","page":{"permalink":"/develbyte/blog/authors/narendra"},"socials":{"github":"https://github.com/im-naren","linkedin":"https://www.linkedin.com/in/im-naren/","email":"mailto:naren.dubey@zoho.com"},"imageURL":"https://github.com/im-naren.png","key":"narendra"}],"frontMatter":{"slug":"how-to-implement-singly-linked-list-in-java","title":"How to Implement Singly Linked List in Java","authors":["narendra"],"tags":["java","coding","data-structure","algorithms"],"date":"2015-05-15T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"How to sort a HashMap on Values","permalink":"/develbyte/blog/how-to-sort-a-hashmap-on-values"},"nextItem":{"title":"Introduction To MapReduce","permalink":"/develbyte/blog/introduction-to-mapreduce"}},"content":"A **Linked List** is a dynamic data structure. The number of nodes in a list is not fixed and can grow and shrink on demand. Any application which has to deal with an unknown number of objects will need to use a linked list.\\n\\nLinked lists and arrays are similar since they both store collections of data. The terminology is that arrays and linked lists store \\"elements\\" on behalf of \\"client\\" code. The specific type of element is not important since essentially the same structure works to store elements of any type. The size of the array is fixed where Linked List is a dynamic and also Inserting new elements at the front or in middle of an array is potentially expensive because existing elements need to be shifted over to make room.\\n\\n## Types of Linked List\\n\\n- Singly Linked List\\n- Doubly Linked List\\n- Circular Linked List\\n\\nSingly Linked List is the most basic types of Linked List where every node holds some data and reference to next Node.\\n\\n![linkedlist](/img/linkedlist.png)\\n\\n## Implementation of Singly Linked List\\n\\n### Node.java\\n\\n```java\\npublic class Node {\\n    \\n    private Node nextNode;\\n    private Object data;\\n\\n    public Node(Object data){ this.data = data;}\\n    public Node(Object data, Node nextNode){ this.data=data;this.nextNode=nextNode;}\\n    public Object getData(){return data;}\\n    public void setData(Object data){this.data=data;}\\n    public Node getNextNode(){return nextNode;}\\n    public void setNextNode(Node nextNode){this.nextNode=nextNode;} \\n}\\n```\\n\\n### LinkedList.java\\n\\n```java\\npublic class LinkedList {\\n    \\n    private int listCount;\\n    private Node head;\\n\\n    public LinkedList() {\\n        head = new Node(null);\\n        listCount = 0;\\n    }\\n\\n    public void add(Object data) {\\n        Node newNode = new Node(data);\\n        Node currentNode = head;\\n        while (currentNode.getNextNode() != null) {\\n            currentNode = currentNode.getNextNode();\\n        }\\n        currentNode.setNextNode(newNode);\\n        listCount++;\\n    }\\n\\n    public void add(Object data, int index) {\\n\\n        if (index <= listCount && index > 0) {\\n            Node newNode = new Node(data);\\n            Node currentNode = head;\\n\\n            for (int i = 1; i < index; i++) {\\n                currentNode = currentNode.getNextNode();\\n            }\\n\\n            newNode.setNextNode(currentNode.getNextNode());\\n            currentNode.setNextNode(newNode);\\n            listCount++;\\n        }\\n    }\\n\\n    public Node get(int index) {\\n\\n        if (index > listCount || index <= 0)\\n            return null;\\n\\n        Node currentNode = head;\\n        for (int i = 1; i <= index; i++) {\\n            currentNode = currentNode.getNextNode();\\n        }\\n        return currentNode;\\n    }\\n\\n    public boolean remove(int index) {\\n\\n        if (index > listCount || index <= 0)\\n            return false;\\n\\n        Node currentNode = head;\\n        for (int i = 0; i < index - 2; i++) {\\n        currentNode = currentNode.getNextNode();\\n        }\\n\\n        currentNode.setNextNode(currentNode.getNextNode().getNextNode());\\n        listCount--;\\n        return true;\\n    }\\n     \\n    public int size() {\\n        return listCount;\\n    }\\n}\\n```\\n\\n### MainClass.java\\n\\n```java\\npublic class testClass {\\n \\n    public static void main(String[] args) {\\n        LinkedList lnklist = new LinkedList();\\n        lnklist.add(\\"naren\\");\\n        lnklist.add(\\"manish\\");\\n        lnklist.add(\\"ram\\");\\n        System.out.println(\\"Number Of Nodes in the List: \\" + lnklist.size());\\n\\n        lnklist.add(\\"naren1\\", 1);\\n        lnklist.add(\\"manish1\\", 3);\\n        lnklist.add(\\"ram1\\", 5);\\n        System.out.println(\\"Number Of Nodes in the List: \\" + lnklist.size());\\n\\n        System.out.println(\\"All the Nodes available in the List : \\");\\n        DisplayList(lnklist);\\n\\n        lnklist.remove(1);\\n        lnklist.remove(3);\\n        lnklist.remove(6);\\n        System.out.println(\\"Number Of Nodes in the List: \\" + lnklist.size());\\n        System.out.println(\\"All the Nodes available in the List : \\");\\n        DisplayList(lnklist);\\n    }\\n \\n    public static void DisplayList(LinkedList ll) {\\n        for (int i = 1; i <= ll.size(); i++) {\\n            System.out.println(i + \\" : \\" + ll.get(i).getData());\\n        }\\n    }\\n}\\n```"},{"id":"introduction-to-mapreduce","metadata":{"permalink":"/develbyte/blog/introduction-to-mapreduce","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2015-05-14-introduction-to-mapreduce.md","source":"@site/blog/2015-05-14-introduction-to-mapreduce.md","title":"Introduction To MapReduce","description":"MapReduce is a framework for processing large amount of data residing on hundreds of computers, its an extraordinarily powerful paradigm. MapReduce was first introduced by Google in 2004 MapReduce: Simplified Data Processing on Large Clusters.","date":"2015-05-14T00:00:00.000Z","tags":[{"inline":false,"label":"Hadoop","permalink":"/develbyte/blog/tags/hadoop","description":"Apache Hadoop ecosystem"},{"inline":false,"label":"MapReduce","permalink":"/develbyte/blog/tags/mapreduce","description":"MapReduce programming model"},{"inline":false,"label":"Big Data","permalink":"/develbyte/blog/tags/big-data","description":"Big data technologies and concepts"},{"inline":false,"label":"Java","permalink":"/develbyte/blog/tags/java","description":"Java programming language"}],"readingTime":1.69,"hasTruncateMarker":false,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","page":{"permalink":"/develbyte/blog/authors/narendra"},"socials":{"github":"https://github.com/im-naren","linkedin":"https://www.linkedin.com/in/im-naren/","email":"mailto:naren.dubey@zoho.com"},"imageURL":"https://github.com/im-naren.png","key":"narendra"}],"frontMatter":{"slug":"introduction-to-mapreduce","title":"Introduction To MapReduce","authors":["narendra"],"tags":["hadoop","mapreduce","big-data","java"],"date":"2015-05-14T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"How to Implement Singly Linked List in Java","permalink":"/develbyte/blog/how-to-implement-singly-linked-list-in-java"},"nextItem":{"title":"HDFS Architecture","permalink":"/develbyte/blog/hdfs-architecture"}},"content":"**MapReduce** is a framework for processing large amount of data residing on hundreds of computers, its an extraordinarily powerful paradigm. MapReduce was first introduced by Google in 2004 MapReduce: Simplified Data Processing on Large Clusters.\\n\\nIn this article we\'ll see how MapReduce processes the data, I am considering the Word Count program as a example, yeah!! this is the worlds most famous MapReduce program!!\\n\\n## Overview\\n\\nThe MapReduce framework operates exclusively on *&lt;key, value&gt;* pairs, that is, the framework views the input to the job as a set of *&lt;key, value&gt;* pairs and produces a set of *&lt;key, value&gt;* pairs as the output of the job, conceivably of different types.\\n\\nThe key and value classes have to be serializable by the framework and hence need to implement the Writable interface. Additionally, the key classes have to implement the WritableComparable interface to facilitate sorting by the framework.\\n\\n## Input and Output types of a MapReduce job:\\n\\n*(input) &lt;k1, v1&gt; -> **map** -> &lt;k2, v2&gt; -> **combine** -> &lt;k2, v2&gt; -> **reduce** -> &lt;k3, v3&gt; (output)*\\n\\n## WordCount.java\\n\\n```java\\nimport java.io.IOException;\\nimport java.util.*;\\n \\nimport org.apache.hadoop.fs.Path;\\nimport org.apache.hadoop.conf.*;\\nimport org.apache.hadoop.io.*;\\nimport org.apache.hadoop.mapred.*;\\nimport org.apache.hadoop.util.*;\\n \\npublic class WordCount {\\n  public static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> {\\n    private final static IntWritable one = new IntWritable(1);\\n    private Text word = new Text();\\n  \\n    public void map(LongWritable key, Text value, \\n                    OutputCollector<Text, IntWritable> output, \\n                    Reporter reporter) throws IOException {\\n      String line = value.toString();\\n      StringTokenizer tokenizer = new StringTokenizer(line);\\n      while (tokenizer.hasMoreTokens()) {\\n        word.set(tokenizer.nextToken());\\n        output.collect(word, one);\\n      }\\n    }\\n  }\\n  \\n  public static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {\\n    public void reduce(Text key, Iterator<IntWritable> values, \\n                        OutputCollector<Text, IntWritable> output, \\n                        Reporter reporter) throws IOException {\\n      int sum = 0;\\n      while (values.hasNext()) {\\n        sum += values.next().get();\\n      }\\n      output.collect(key, new IntWritable(sum));\\n    }\\n  }\\n  \\n  public static void main(String[] args) throws Exception {\\n    JobConf conf = new JobConf(WordCount.class);\\n    conf.setJobName(\\"wordcount\\");\\n    \\n    conf.setOutputKeyClass(Text.class);\\n    conf.setOutputValueClass(IntWritable.class);\\n    \\n    conf.setMapperClass(Map.class);\\n    conf.setCombinerClass(Reduce.class);\\n    conf.setReducerClass(Reduce.class);\\n    \\n    conf.setInputFormat(TextInputFormat.class);\\n    conf.setOutputFormat(TextOutputFormat.class);\\n    \\n    FileInputFormat.setInputPaths(conf, new Path(args[0]));\\n    FileOutputFormat.setOutputPath(conf, new Path(args[1]));\\n    \\n    JobClient.runJob(conf);\\n  }\\n}\\n```"},{"id":"hdfs-architecture","metadata":{"permalink":"/develbyte/blog/hdfs-architecture","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2015-05-03-hdfs-architecture.md","source":"@site/blog/2015-05-03-hdfs-architecture.md","title":"HDFS Architecture","description":"The Hadoop Distributed File System (HDFS) is a highly fault tolerant file system designed and optimized to be deployed on a distributed infrastructure established with a bunch commodity hardware. HDFS provides high throughput access to application data and is best suited for applications that have large data sets. Unlike existing distributed file systems HDFS have loosen up a few POSIX Standards to enable streaming access to file system data. HDFS was originally developed as an infrastructure for the Apache Nutch web search engine project.","date":"2015-05-03T00:00:00.000Z","tags":[{"inline":false,"label":"Hadoop","permalink":"/develbyte/blog/tags/hadoop","description":"Apache Hadoop ecosystem"},{"inline":false,"label":"HDFS","permalink":"/develbyte/blog/tags/hdfs","description":"Hadoop Distributed File System"},{"inline":false,"label":"Big Data","permalink":"/develbyte/blog/tags/big-data","description":"Big data technologies and concepts"},{"inline":false,"label":"Architecture","permalink":"/develbyte/blog/tags/architecture","description":"System and software architecture"}],"readingTime":1.93,"hasTruncateMarker":false,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","page":{"permalink":"/develbyte/blog/authors/narendra"},"socials":{"github":"https://github.com/im-naren","linkedin":"https://www.linkedin.com/in/im-naren/","email":"mailto:naren.dubey@zoho.com"},"imageURL":"https://github.com/im-naren.png","key":"narendra"}],"frontMatter":{"slug":"hdfs-architecture","title":"HDFS Architecture","authors":["narendra"],"tags":["hadoop","hdfs","big-data","architecture"],"date":"2015-05-03T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Introduction To MapReduce","permalink":"/develbyte/blog/introduction-to-mapreduce"},"nextItem":{"title":"MapReduce Execution in Hadoop","permalink":"/develbyte/blog/mapreduce-execution-in-hadoop"}},"content":"The Hadoop Distributed File System (HDFS) is a highly fault tolerant file system designed and optimized to be deployed on a distributed infrastructure established with a bunch commodity hardware. HDFS provides high throughput access to application data and is best suited for applications that have large data sets. Unlike existing distributed file systems HDFS have loosen up a few POSIX Standards to enable streaming access to file system data. HDFS was originally developed as an infrastructure for the Apache Nutch web search engine project.\\n\\n## Inside HDFS\\n\\n![hdfs-architecture.png](/img/hdfs-architecture.png)\\n\\nHDFS is based on Master-Slave Architecture. A typical HDFS cluster consists one NameNode and multiple DataNodes generally one per node in the cluster. The NameNode is the arbitrator and repository for all **HDFS metadata**. The system is designed in such a way that user data never flows through the NameNode. NameNode acts as the master in Master-Slave Architectural pattern and manages the file system namespace and regulates access to files by client Applications. DataNode manages the user data stored on the node that they run on. A file is split into one or more blocks and set of blocks are stored in DataNodes. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes from the heartbeats and block reports sent by DataNodes. DataNodes executes read, write requests and performs block creation, block deletion, and block replication only when NameNode commands them to.\\n\\n## Deployment\\n\\nNameNode and DataNode are software programs designed in java, so it can execute on any machine having java installed (typically Unix/Linux), it can be deployed on a wide range of machines. In real time scenarios NameNodes are deployed on a high end dedicated machines and all other machines in the cluster are commodity hardware running DataNode program. Each cluster consists of one NameNode and multiple DataNodes. There could be multiple NameNodes within a cluster in some special scenarios, In order to scale name services horizontally federation uses multiple independent NameNodes within a cluster these NameNodes does not require coordination between each other. The DataNodes are used as common storage among the NameNodes, and all the DataNodes are required to be registered with all the NameNodes and send periodic heartbeats and block reports to the NameNodes."},{"id":"mapreduce-execution-in-hadoop","metadata":{"permalink":"/develbyte/blog/mapreduce-execution-in-hadoop","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2015-03-10-mapreduce-execution-in-hadoop.md","source":"@site/blog/2015-03-10-mapreduce-execution-in-hadoop.md","title":"MapReduce Execution in Hadoop","description":"In this article we have tried to summaries,  how a MapReduce program executes in Hadoop environment.","date":"2015-03-10T00:00:00.000Z","tags":[{"inline":false,"label":"Hadoop","permalink":"/develbyte/blog/tags/hadoop","description":"Apache Hadoop ecosystem"},{"inline":false,"label":"MapReduce","permalink":"/develbyte/blog/tags/mapreduce","description":"MapReduce programming model"},{"inline":false,"label":"Big Data","permalink":"/develbyte/blog/tags/big-data","description":"Big data technologies and concepts"}],"readingTime":1.07,"hasTruncateMarker":false,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","page":{"permalink":"/develbyte/blog/authors/narendra"},"socials":{"github":"https://github.com/im-naren","linkedin":"https://www.linkedin.com/in/im-naren/","email":"mailto:naren.dubey@zoho.com"},"imageURL":"https://github.com/im-naren.png","key":"narendra"}],"frontMatter":{"slug":"mapreduce-execution-in-hadoop","title":"MapReduce Execution in Hadoop","authors":["narendra"],"tags":["hadoop","mapreduce","big-data"],"date":"2015-03-10T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"HDFS Architecture","permalink":"/develbyte/blog/hdfs-architecture"},"nextItem":{"title":"Difference between Unix and Linux","permalink":"/develbyte/blog/difference-between-unix-and-linux"}},"content":"In this article we have tried to summaries,  how a MapReduce program executes in Hadoop environment.\\n\\n## MapReduce 1 Execution Sequence:\\n\\nMapReduce execution starts with below command.\\n\\n**Step 1:** `$ hadoop jar <jar> [mainClass] args...`\\n\\nThis command starts the MapReduce execution in Clients JVM.\\ncreates the Job.\\n\\n**Step 2:** `JobTracker.getNewJobId()`\\n\\nClient Asks JobTracker for a new JobId.\\n\\n**Step 3:** `JobClient.SubmitJob() / JobClient.runJob()`\\n\\n- Checking the input and output specifications of the job.\\n- Computing the InputSplits for the job.\\n- Setup the requisite accounting information for the DistributedCache of the job, if necessary.\\n- Copying the job\'s jar and configuration to the JobTracker  file-system, in a folder names as the JobId assigned with very high replication factor(default 10).\\n- Submitting the job to the JobTracker and optionally monitoring it\'s status.\\n- JobTracker puts the job into an internal queue from where JobScheduler will pick it up and Initialize\\n\\n**Step 4:** Initialize the Job\\n\\n- Creates an object of the Job.\\n- Encapsulates its Tasks.\\n- Retrieve the InputSplit and create one map task for each split.\\n- Creates the reduces task the number of reduce task depends on the number defined in the driver code(default 1).\\n- Creates a Job Setup task to setup the job before map tasks run.\\n- Creates a Job Cleanup task to run after the reducer task run.\\n\\n![mr1_execution](/img/mr1_execution.png)"},{"id":"difference-between-unix-and-linux","metadata":{"permalink":"/develbyte/blog/difference-between-unix-and-linux","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2015-03-07-difference-between-unix-and-linux.md","source":"@site/blog/2015-03-07-difference-between-unix-and-linux.md","title":"Difference between Unix and Linux","description":"Back in 1969, UNIX has evolved through a number of different versions and environment. One of the Original UNIX editors has licensed the most modern UNIX variants. It\'s closed Source software. There are various flavors of UNIX are available in market like Sun\'s Solaris, Hewlett-Packard\'s HP-UX, and IBM\'s AIX all of these have their own Unique Foundation, all these flavors are optimized and incorporated with different tools which are most compatible with their hardware.","date":"2015-03-07T00:00:00.000Z","tags":[{"inline":false,"label":"Unix","permalink":"/develbyte/blog/tags/unix","description":"Unix operating system"},{"inline":false,"label":"Linux","permalink":"/develbyte/blog/tags/linux","description":"Linux operating system"},{"inline":false,"label":"Operating Systems","permalink":"/develbyte/blog/tags/operating-systems","description":"Operating system concepts and comparisons"},{"inline":false,"label":"Hadoop","permalink":"/develbyte/blog/tags/hadoop","description":"Apache Hadoop ecosystem"}],"readingTime":2.54,"hasTruncateMarker":false,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","page":{"permalink":"/develbyte/blog/authors/narendra"},"socials":{"github":"https://github.com/im-naren","linkedin":"https://www.linkedin.com/in/im-naren/","email":"mailto:naren.dubey@zoho.com"},"imageURL":"https://github.com/im-naren.png","key":"narendra"}],"frontMatter":{"slug":"difference-between-unix-and-linux","title":"Difference between Unix and Linux","authors":["narendra"],"tags":["unix","linux","operating-systems","hadoop"],"date":"2015-03-07T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"MapReduce Execution in Hadoop","permalink":"/develbyte/blog/mapreduce-execution-in-hadoop"},"nextItem":{"title":"How to Find out Next and Previous Day of Week in Oracle","permalink":"/develbyte/blog/how-to-find-out-next-and-previous-day-of-week-in-oracle"}},"content":"Back in 1969, UNIX has evolved through a number of different versions and environment. One of the Original UNIX editors has licensed the most modern UNIX variants. It\'s closed Source software. There are various flavors of UNIX are available in market like Sun\'s Solaris, Hewlett-Packard\'s HP-UX, and IBM\'s AIX all of these have their own Unique Foundation, all these flavors are optimized and incorporated with different tools which are most compatible with their hardware.\\n\\nLinux is an open source (free to use and redistribute under GNU licenses) operating system widely used for computer hardware and software, game development, tablet PCS, mainframes etc. UNIX is a copyrighted name only big companies (IBM, HP etc\u2026) are allowed to use the UNIX copyright and names. UNIX is commonly used in internet servers, workstations and PCs by Solaris, Intel, and HP etc.\\n\\nLinux Kernel was designed by Linus Torvalds with an intention of developing a Unix-like operating system as an open source alternative of UNIX environment.  An administrator or developer who supports Linux systems might find it uncomfortable to move to a commercial UNIX system. On the whole, the foundations of any UNIX-like operating system (tools, filesystem layout, programming APIs) are fairly standardized. However, some details of the systems show significant differences. The remainder of this article covers the details of these differences.\\n\\n## In The Box\\n\\nLinux is designed as just kernel. All Linux distributions package includes GUI system, GNU utilities, installation & management tools, GNU C/C++ Compilers, Editors and various applications (e.g. OpenOffice, Firefox). However, most UNIX operating systems come as A-Z package everything is designed and distributed by the same vender.\\n\\n## Interface\\n\\nLinux is considered as most user friendly UNIX like operating systems. It makes it easy to install sound card, flash players, and other desktop goodies. However, Apple OS X is most popular UNIX operating system for desktop usage.\\n\\n## Filesystem support\\n\\nOne of the reasons Linux has become such a powerful tool is its immense support for other operating systems. One of the most popular features is the plethora of filesystems that are available. Most commercial version of UNIX supports two, or possibly three, different local filesystem types. Linux, however, supports almost all of the filesystems that are currently available on any operating system.\\n\\n## Usages\\n\\nLinux can be installed on a wide variety of computer hardware, ranging from mobile phones, tablet computers and video game consoles, to mainframes and supercomputers where as The UNIX operating system is used in internet servers, workstations & PCs. Backbone of the majority of finance infrastructure and many 24\xd7365 high availability solutions.\\n\\n## In Short\\n\\nClearly the general environment between UNIX and Linux is very similar. However they have differences in their filesystem and Kernel, but overall these differences are not major. They might need specialization in optimizing the work, but since the underlying concepts are so mature that changes are very rare. There are so many differences in UNIX itself among its variants that UNIX and Linux feel like they are one of the variant of each other."},{"id":"how-to-find-out-next-and-previous-day-of-week-in-oracle","metadata":{"permalink":"/develbyte/blog/how-to-find-out-next-and-previous-day-of-week-in-oracle","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2015-02-03-how-to-find-out-next-and-previous-day-of-week-in-oracle.md","source":"@site/blog/2015-02-03-how-to-find-out-next-and-previous-day-of-week-in-oracle.md","title":"How to Find out Next and Previous Day of Week in Oracle","description":"I have seen people writing number of lines of code to find out the date on a specific day in current or previous week, most often we need Friday the last working day of the week. We have a pretty simple way to find out in Oracle Queries.","date":"2015-02-03T00:00:00.000Z","tags":[{"inline":false,"label":"Oracle","permalink":"/develbyte/blog/tags/oracle","description":"Oracle database related content"},{"inline":false,"label":"SQL","permalink":"/develbyte/blog/tags/sql","description":"SQL queries and database operations"},{"inline":false,"label":"Database","permalink":"/develbyte/blog/tags/database","description":"Database design and management"},{"inline":false,"label":"Hadoop","permalink":"/develbyte/blog/tags/hadoop","description":"Apache Hadoop ecosystem"}],"readingTime":1.05,"hasTruncateMarker":false,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","page":{"permalink":"/develbyte/blog/authors/narendra"},"socials":{"github":"https://github.com/im-naren","linkedin":"https://www.linkedin.com/in/im-naren/","email":"mailto:naren.dubey@zoho.com"},"imageURL":"https://github.com/im-naren.png","key":"narendra"}],"frontMatter":{"slug":"how-to-find-out-next-and-previous-day-of-week-in-oracle","title":"How to Find out Next and Previous Day of Week in Oracle","authors":["narendra"],"tags":["oracle","sql","database","hadoop"],"date":"2015-02-03T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Difference between Unix and Linux","permalink":"/develbyte/blog/difference-between-unix-and-linux"},"nextItem":{"title":"Welcome to Develbyte","permalink":"/develbyte/blog/welcome-to-develbyte"}},"content":"I have seen people writing number of lines of code to find out the date on a specific day in current or previous week, most often we need Friday the last working day of the week. We have a pretty simple way to find out in Oracle Queries.\\n\\n## Logic:\\n- Add a number of the day you want the date for in your current date, assuming Monday as day 1, Tuesday as day 2 and so on.\\n\\n```sql\\nTRUNC(TO_DATE(SYSDATE,\'YYYYMMDD\') + 1, \'d\')\\n```\\n\\nSince Oracle considers Sunday as the first day of the week and Saturday as the last day adding 1 will shift this week frame one day back now week starts from Saturday and ends on Friday and TRUNC will give the currents weeks Sunday.\\n\\n- Now whatever day you want in the week just add or subtract the number of day from the Sunday you got. In our case we are looking for Friday so we add 5 since it is the 5 day of our new week.\\n\\n```sql\\nTRUNC(TO_DATE(SYSDATE,\'YYYYMMDD\') + 1, \'d\') + 5\\n```\\n\\nSo our Query will look something like this\\n\\n```sql\\nSELECT TRUNC(TO_DATE(SYSDATE,\'YYYYMMDD\') + 1, \'d\') + 52\\nFROM DUAL;\\n```\\n\\n**Output:**\\n\\n![previous-day-of-week-sql](/img/previous-day-of-week-sql.png)"},{"id":"welcome-to-develbyte","metadata":{"permalink":"/develbyte/blog/welcome-to-develbyte","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2014-03-11-welcome-to-develbyte.md","source":"@site/blog/2014-03-11-welcome-to-develbyte.md","title":"Welcome to Develbyte","description":"You\'ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.","date":"2014-03-11T00:00:00.000Z","tags":[{"inline":false,"label":"Blog","permalink":"/develbyte/blog/tags/blog","description":"Blog-related content and updates"},{"inline":false,"label":"First Post","permalink":"/develbyte/blog/tags/first-post","description":"Initial blog posts"}],"readingTime":0.92,"hasTruncateMarker":false,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","page":{"permalink":"/develbyte/blog/authors/narendra"},"socials":{"github":"https://github.com/im-naren","linkedin":"https://www.linkedin.com/in/im-naren/","email":"mailto:naren.dubey@zoho.com"},"imageURL":"https://github.com/im-naren.png","key":"narendra"}],"frontMatter":{"slug":"welcome-to-develbyte","title":"Welcome to Develbyte","authors":["narendra"],"tags":["blog","first-post"],"date":"2014-03-11T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"How to Find out Next and Previous Day of Week in Oracle","permalink":"/develbyte/blog/how-to-find-out-next-and-previous-day-of-week-in-oracle"}},"content":"You\'ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run `jekyll serve`, which launches a web server and auto-regenerates your site when a file is updated.\\n\\nTo add new posts, simply add a file in the `_posts` directory that follows the convention `YYYY-MM-DD-name-of-post.ext` and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.\\n\\nJekyll also offers powerful support for code snippets:\\n\\n```ruby\\ndef print_hi(name)\\n  puts \\"Hi, #{name}\\"\\nend\\nprint_hi(\'Tom\')\\n#=> prints \'Hi, Tom\' to STDOUT.\\n```\\n\\nCheck out the [Jekyll docs][jekyll-docs] for more info on how to get the most out of Jekyll. File all bugs/feature requests at [Jekyll\'s GitHub repo][jekyll-gh]. If you have questions, you can ask them on [Jekyll Talk][jekyll-talk].\\n\\n[jekyll-docs]: https://jekyllrb.com/docs/home\\n[jekyll-gh]:   https://github.com/jekyll/jekyll\\n[jekyll-talk]: https://talk.jekyllrb.com/"}]}}')}}]);