"use strict";(self.webpackChunkmy_blog=self.webpackChunkmy_blog||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"modern-web-development-trends-2024","metadata":{"permalink":"/blog/modern-web-development-trends-2024","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-01-20-modern-web-development-trends-2024.md","source":"@site/blog/2024-01-20-modern-web-development-trends-2024.md","title":"Modern Web Development Trends in 2024","description":"The web development landscape continues to evolve rapidly. Here are the key trends shaping modern web development in 2024.","date":"2024-01-20T00:00:00.000Z","tags":[{"inline":true,"label":"web-development","permalink":"/blog/tags/web-development"},{"inline":true,"label":"trends","permalink":"/blog/tags/trends"},{"inline":true,"label":"react","permalink":"/blog/tags/react"},{"inline":true,"label":"typescript","permalink":"/blog/tags/typescript"},{"inline":true,"label":"ai","permalink":"/blog/tags/ai"}],"readingTime":1.13,"hasTruncateMarker":true,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","imageURL":"https://github.com/im-naren.png","key":"narendra","page":null}],"frontMatter":{"slug":"modern-web-development-trends-2024","title":"Modern Web Development Trends in 2024","authors":["narendra"],"tags":["web-development","trends","react","typescript","ai"]},"unlisted":false,"nextItem":{"title":"Getting Started with Docusaurus - Building Your First Blog","permalink":"/blog/getting-started-with-docusaurus"}},"content":"The web development landscape continues to evolve rapidly. Here are the key trends shaping modern web development in 2024.\\n\\n\x3c!-- truncate --\x3e\\n\\n## 1. AI-Powered Development\\n\\nArtificial Intelligence is revolutionizing how we build web applications:\\n\\n- **AI Code Assistants**: Tools like GitHub Copilot and ChatGPT are becoming essential\\n- **Automated Testing**: AI-generated test cases and bug detection\\n- **Code Generation**: From design to implementation using AI tools\\n\\n## 2. Edge Computing\\n\\nMoving computation closer to users for better performance:\\n\\n- **Edge Functions**: Serverless functions running at the edge\\n- **CDN Evolution**: More than just static content delivery\\n- **Reduced Latency**: Faster response times for global users\\n\\n## 3. WebAssembly (WASM)\\n\\nBringing near-native performance to the web:\\n\\n- **Performance**: Running C/C++/Rust code in browsers\\n- **New Use Cases**: Games, image processing, scientific computing\\n- **Framework Integration**: Better tooling and framework support\\n\\n## 4. Micro-Frontends\\n\\nBreaking down monolithic frontend applications:\\n\\n- **Team Autonomy**: Independent development and deployment\\n- **Technology Diversity**: Different teams can use different frameworks\\n- **Scalability**: Easier to scale large applications\\n\\n## 5. TypeScript Everywhere\\n\\nTypeScript continues to dominate:\\n\\n- **Better Developer Experience**: Improved tooling and error catching\\n- **Framework Support**: Excellent support in React, Vue, Angular\\n- **Enterprise Adoption**: Growing adoption in large organizations\\n\\n## Conclusion\\n\\nThese trends are shaping the future of web development. Staying updated with these technologies will help you build better, more efficient web applications.\\n\\nWhat trends are you most excited about? Let me know in the comments!"},{"id":"getting-started-with-docusaurus","metadata":{"permalink":"/blog/getting-started-with-docusaurus","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-01-15-getting-started-with-docusaurus.md","source":"@site/blog/2024-01-15-getting-started-with-docusaurus.md","title":"Getting Started with Docusaurus - Building Your First Blog","description":"Docusaurus is a powerful static site generator that makes it easy to create documentation sites and blogs. In this post, we\'ll explore how to set up your first Docusaurus blog and customize it to match your brand.","date":"2024-01-15T00:00:00.000Z","tags":[{"inline":true,"label":"docusaurus","permalink":"/blog/tags/docusaurus"},{"inline":true,"label":"blog","permalink":"/blog/tags/blog"},{"inline":true,"label":"web-development","permalink":"/blog/tags/web-development"},{"inline":true,"label":"tutorial","permalink":"/blog/tags/tutorial"}],"readingTime":1.01,"hasTruncateMarker":true,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","imageURL":"https://github.com/im-naren.png","key":"narendra","page":null}],"frontMatter":{"slug":"getting-started-with-docusaurus","title":"Getting Started with Docusaurus - Building Your First Blog","authors":["narendra"],"tags":["docusaurus","blog","web-development","tutorial"]},"unlisted":false,"prevItem":{"title":"Modern Web Development Trends in 2024","permalink":"/blog/modern-web-development-trends-2024"},"nextItem":{"title":"Zookeeper Sessions and life cycle","permalink":"/blog/zookeeper-sessions-and-life-cycle"}},"content":"Docusaurus is a powerful static site generator that makes it easy to create documentation sites and blogs. In this post, we\'ll explore how to set up your first Docusaurus blog and customize it to match your brand.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Why Docusaurus?\\n\\nDocusaurus offers several advantages for creating blogs and documentation sites:\\n\\n- **Easy Setup**: Get started quickly with minimal configuration\\n- **React-based**: Leverage the power of React for custom components\\n- **SEO Optimized**: Built-in SEO features for better search engine visibility\\n- **Mobile Responsive**: Works great on all device sizes\\n- **Plugin Ecosystem**: Extend functionality with plugins\\n\\n## Getting Started\\n\\nTo create a new Docusaurus site, run:\\n\\n```bash\\nnpx create-docusaurus@latest my-blog classic\\n```\\n\\nThis command will create a new Docusaurus site with the classic template, which includes both documentation and blog functionality.\\n\\n## Customization\\n\\nOnce you have your site set up, you can customize it by:\\n\\n1. **Updating the configuration** in `docusaurus.config.ts`\\n2. **Adding custom CSS** in `src/css/custom.css`\\n3. **Creating custom components** in `src/components/`\\n4. **Adding blog posts** in the `blog/` directory\\n\\n## Conclusion\\n\\nDocusaurus is an excellent choice for creating blogs and documentation sites. With its easy setup and powerful customization options, you can create a professional-looking site in no time.\\n\\nHappy blogging!"},{"id":"zookeeper-sessions-and-life-cycle","metadata":{"permalink":"/blog/zookeeper-sessions-and-life-cycle","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2016-11-04-zookeeper-sessions-and-life-cycle.md","source":"@site/blog/2016-11-04-zookeeper-sessions-and-life-cycle.md","title":"Zookeeper Sessions and life cycle","description":"Session and request order handling:","date":"2016-11-04T00:00:00.000Z","tags":[{"inline":true,"label":"zookeeper","permalink":"/blog/tags/zookeeper"},{"inline":true,"label":"distributed-computing","permalink":"/blog/tags/distributed-computing"},{"inline":true,"label":"kafka","permalink":"/blog/tags/kafka"},{"inline":true,"label":"coordination","permalink":"/blog/tags/coordination"}],"readingTime":1.65,"hasTruncateMarker":true,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","imageURL":"https://github.com/im-naren.png","key":"narendra","page":null}],"frontMatter":{"slug":"zookeeper-sessions-and-life-cycle","title":"Zookeeper Sessions and life cycle","authors":["narendra"],"tags":["zookeeper","distributed-computing","kafka","coordination"],"date":"2016-11-04T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Getting Started with Docusaurus - Building Your First Blog","permalink":"/blog/getting-started-with-docusaurus"},"nextItem":{"title":"Zookeeper Namespace And Operations","permalink":"/blog/zookeeper-namespace-and-operations"}},"content":"## Session and request order handling:\\n\\nSessions is very important and quite critical for the operation of ZooKeeper. All operations a client submits to ZooKeeper are associated to a session. When a session ends for any reason, the ephemeral nodes created during that session disappear.\\n\\nThe client initially connects to any server in the ensemble, and only to a single server. It uses a TCP connection to communicate with the server, but the session may be moved to a different server if the client has not heard from its current server for some time. Moving a session to a different server is handled transparently by the ZooKeeper client library.\\n\\n\x3c!-- truncate --\x3e\\n\\nSessions offer order guarantees, which means that requests in a session are executed in FIFO (first in, first out) order. Typically, a client has only a single session open, so its requests are all executed in FIFO order. When a client creates a ZooKeeper handle using a specific language binding, it establishes a session with the service. If a client has multiple concurrent sessions, FIFO ordering is not necessarily preserved across the sessions. Consecutive sessions of the same client, even if they don\'t overlap in time, also do not necessarily preserve FIFO order.\\n\\n### Here is how it can happen in this case:\\n- Client establishes a session and makes two consecutive asynchronous calls to `create /tasks` and `create /workers`.\\n- First session expires.\\n- Client establishes another session and makes an asynchronous call to `create /assign`.\\nIn this sequence of calls, it is possible that only `/tasks` and `/assign` have been created, which preserves FIFO ordering for the first session but violates it across sessions.\\n\\n## States and the Lifetime of a Session\\n\\nThe lifetime of a session is the period between its creation and its end, whether it is closed gracefully or expires because of a timeout. The possible states of a session are : *CONNECTING*, *CONNECTED*, *CLOSED*, and *NOT_CONNECTED*.\\n\\n![states-and-the-Lifetime-of-a-Session](/img/states-and-the-Lifetime-of-a-Session.png)"},{"id":"zookeeper-namespace-and-operations","metadata":{"permalink":"/blog/zookeeper-namespace-and-operations","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2016-10-27-zookeeper-namespace-and-operations.md","source":"@site/blog/2016-10-27-zookeeper-namespace-and-operations.md","title":"Zookeeper Namespace And Operations","description":"Zookeper data Model:","date":"2016-10-27T00:00:00.000Z","tags":[{"inline":true,"label":"zookeeper","permalink":"/blog/tags/zookeeper"},{"inline":true,"label":"distributed-computing","permalink":"/blog/tags/distributed-computing"},{"inline":true,"label":"kafka","permalink":"/blog/tags/kafka"},{"inline":true,"label":"coordination","permalink":"/blog/tags/coordination"}],"readingTime":4.63,"hasTruncateMarker":true,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","imageURL":"https://github.com/im-naren.png","key":"narendra","page":null}],"frontMatter":{"slug":"zookeeper-namespace-and-operations","title":"Zookeeper Namespace And Operations","authors":["narendra"],"tags":["zookeeper","distributed-computing","kafka","coordination"],"date":"2016-10-27T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Zookeeper Sessions and life cycle","permalink":"/blog/zookeeper-sessions-and-life-cycle"},"nextItem":{"title":"Zookeeper Introduction to Zookeeper","permalink":"/blog/zookeeper-introduction-to-zookeeper"}},"content":"## Zookeper data Model:\\n\\nZooKeeper has a hierarchal name space(as shown below), much like a distributed file system. The only difference is that each node in the namespace can have data associated with it as well as children. It is like having a file system that allows a file to also be a directory.\\n\\n![zookeeper-data-model](/img/zookeeper-data-model.png)\\n\\n\x3c!-- truncate --\x3e\\n\\nThe root node contains four more nodes, and three of those nodes have nodes under them. The leaf nodes are the data.\\n\\n## Znode:\\n\\nEvery node in a ZooKeeper tree is referred to as a znode.Znodes maintain a stat structure that includes version numbers for data changes, acl changes. The stat structure also has timestamps. The version number, together with the timestamp, allows ZooKeeper to validate the cache and to coordinate updates. Each time a znode\'s data changes, the version number increases.\\n\\nZnodes may or may not contain data. If a znode contains any data, the data is stored as a  byte  array.  The  exact  format  of  the  byte  array  is  specific  to  each  application,  and ZooKeeper does not directly provide support to parse it. The absence of data often conveys important information about a znode. For Example :- in a typical master-worker application the absence of a master znode means that no master is currently elected.\\n\\n### The ZooKeeper API exposes the following operations:\\n\\n- `create /path data` - Creates a znode named with /path  and containing data\\n- `delete /path` - Deletes the znode  /path\\n- `exists /path` - Checks whether /path exists\\n- `setData /path data` - Sets the data of znode /path to  data\\n- `getData /path` - Returns the data in /path\\n- `getChildren /path` - Returns the list of children under /path\\n\\nOne important note is that ZooKeeper does not allow partial writes or reads of the znode data. When setting the data of a znode or reading it, the content of the znode is replaced or read entirely.\\n\\n## Different Modes of Znode:\\n\\nZnodes can be created with four different modes: persistent, ephemeral, presistent_sequential and ephemeral_sequential\\n\\n### Persistent and ephemeral znodes\\n\\nA znode can be either persistent or ephemeral. A persistent znode /path can be deleted only through a call to delete. An ephemeral znode, in contrast, is deleted if the client that created it crashes or simply closes its connection to ZooKeeper.\\n\\nPersistent znodes are useful when the znode stores some data on behalf of an application and this data needs to be preserved even after its creator is no longer part of the system.\\n\\nEphemeral znodes convey information about some aspect of the application that must exist only while the session of its creator is valid.\\n\\n### Sequential znodes:\\n\\nA sequential znode is assigned a unique, monotonically increasing integer. This sequence number is appended to the path used to create the znode. For example, if a client creates a sequential znode with the path /tasks/ task-, ZooKeeper assigns a sequence number, say 1, and appends it to the path. The path of the znode becomes /tasks/task-1. Sequential znodes provide an easy way to create znodes with unique names. They also provide a way to easily see the creation order of znodes.\\n\\n## Version:\\n\\nEach znode has a version number associated with it that is incremented every time its data changes. A couple of operations in the API can be executed conditionally: setData and delete. Both calls take a version as an input parameter, and the operation succeeds only if the version passed by the client matches the current version on the server. The use of versions is important when multiple ZooKeeper clients might be trying to perform operations over the same znode.\\n\\n## Watch:\\n\\nA watch is a one-shot operation, which means that it triggers one notification for any changes to znodes. Registering to receive a notification for a given znode consists of setting a watch. To receive multiple notifications over time, the client must set a new watch upon receiving each notification.\\n\\n## Data Access:\\n\\nThe data stored at each znode in a namespace is read and written atomically. Reads get all the data bytes associated with a znode and a write replaces all the data. Each node has an Access Control List (ACL) that restricts who can do what.\\n\\nZooKeeper was not designed to be a general database or large object store. Instead, it manages coordination data. This data can come in the form of configuration, status information, rendezvous, etc.\\n\\n## Semantics of Watches :\\n\\nWe can set watches with the three calls that read the state of ZooKeeper: `exists`, `getData`, and `getChildren`.The following list details the events that a watch can trigger and the calls that enable them:\\n\\n- **Created event**: Enabled with a call to exists.\\n- **Deleted event**: Enabled with a call to exists, `getData`, and `getChildren`.\\n- **Changed event**: Enabled with a call to exists and `getData`.\\n- **Child event**: Enabled with a call to `getChildren`.\\n\\n## Remove Watches:\\n\\nWe can remove the watches registered on a znode with a call to `removeWatches`. Also, a ZooKeeper client can remove watches locally even if there is no server connection by setting the local flag to true. The following list details the events which will be triggered after the successful watch removal.\\n\\n- **Child Remove event**: Watcher which was added with a call to `getChildren`.\\n- **Data Remove event**: Watcher which was added with a call to exists or `getData`.\\n\\n## ACL Permissions :\\n\\nZooKeeper supports the following permissions:\\n\\n- `CREATE`: you can create a child node\\n- `READ`: you can get data from a node and list its children.\\n- `WRITE`: you can set data for a node\\n- `DELETE`: you can delete a child node\\n- `ADMIN`: you can set permissions"},{"id":"zookeeper-introduction-to-zookeeper","metadata":{"permalink":"/blog/zookeeper-introduction-to-zookeeper","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2016-10-20-zookeeper-introduction-to-zookeeper.md","source":"@site/blog/2016-10-20-zookeeper-introduction-to-zookeeper.md","title":"Zookeeper Introduction to Zookeeper","description":"Zookeper is an open-source, centralised co-ordination service which is used to co-ordinate the services and manage the configurations of applications accross a large number of hosts over a distributed environment.","date":"2016-10-20T00:00:00.000Z","tags":[{"inline":true,"label":"zookeeper","permalink":"/blog/tags/zookeeper"},{"inline":true,"label":"distributed-computing","permalink":"/blog/tags/distributed-computing"},{"inline":true,"label":"kafka","permalink":"/blog/tags/kafka"},{"inline":true,"label":"coordination","permalink":"/blog/tags/coordination"}],"readingTime":2.18,"hasTruncateMarker":true,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","imageURL":"https://github.com/im-naren.png","key":"narendra","page":null}],"frontMatter":{"slug":"zookeeper-introduction-to-zookeeper","title":"Zookeeper Introduction to Zookeeper","authors":["narendra"],"tags":["zookeeper","distributed-computing","kafka","coordination"],"date":"2016-10-20T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Zookeeper Namespace And Operations","permalink":"/blog/zookeeper-namespace-and-operations"},"nextItem":{"title":"Message Queue","permalink":"/blog/message-queue"}},"content":"**Zookeper** is an open-source, centralised co-ordination service which is used to co-ordinate the services and manage the configurations of applications accross a large number of hosts over a distributed environment.\\n\\nCo-ordinating between the services in a distributed application is a complex process. ZooKeeper was designed to be a robust service that enables application developers to focus mainly on their  application logic rather than coordination. It exposes a simple API, similar to filesystem API, that allows developers to implement common co\u2010ordination tasks, such as electing a master server,managing group membership, and managing metadata.\\n\\nZookeper is open-sorced to Apache by Yahoo. Apache Zookeper have became standard for organising the services in Hadoop, kafka and other distributed frameworks.\\n\\n\x3c!-- truncate --\x3e\\n\\n## What is a distributed application?\\n\\nDistributed applications are the type of application comprised of multiple software components running independently and concurrently across multiple physical machines. Distributed applications are required to do resource intensive computions which is difficult or impossible to do in a non-distributed environment. The group is machines running the distributed applications are togethere called as cluster and each machine individually called as Nodes. Distributed applications provides scalability and performance along with other benifits, But it can also easly lead to various complexities like Race Condition, DeadLock, Inconsistency, This is where Zookeeper comes to rescue. Zookeper helps to create co-ordianation between the tasks and maintain shared data with robust syncronisation technique.\\n\\nLet\'s look at some examples where ZooKeeper has been useful to get a better sense of where it is applicable:\\n\\n## 1. Apache HBase\\nHBase is a data store typically used alongside Hadoop. In HBase, ZooKeeper is used to  elect  a  cluster  master,  to  keep  track  of  available  servers,  and  to  keep  cluster metadata.\\n\\n## 2. Apache Kafka\\nKafka is a pub-sub messaging system. It uses ZooKeeper to detect crashes, to implement topic discovery, and to maintain production and consumption state for topics.\\n\\n## 3. Apache Solr\\nSolr is an enterprise search platform. In its distributed form, called SolrCloud, it uses ZooKeeper to store metadata about the cluster and coordinate the updates to this metadata.\\n\\n## 4. Yahoo! Fetching Service\\nPart of a crawler implementation, the Fetching Service  fetches web pages efficiently by  caching content while making sure that web server policies, such as those in robots.txt files, are preserved. This service uses ZooKeeper for tasks such as master election, crash detection, and metadata storage.\\n\\n## 5. Facebook Messages\\nThis is a Facebook application that integrates communication channels: email, SMS, Facebook Chat, and the existing Facebook Inbox. It uses ZooKeeper as a controller for implementing sharding and failover, and also for service discovery."},{"id":"message-queue","metadata":{"permalink":"/blog/message-queue","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2016-09-20-message-queue.md","source":"@site/blog/2016-09-20-message-queue.md","title":"Message Queue","description":"What is Message Passing?","date":"2016-09-20T00:00:00.000Z","tags":[{"inline":true,"label":"big-data","permalink":"/blog/tags/big-data"},{"inline":true,"label":"kafka","permalink":"/blog/tags/kafka"},{"inline":true,"label":"message-queue","permalink":"/blog/tags/message-queue"},{"inline":true,"label":"distributed-systems","permalink":"/blog/tags/distributed-systems"}],"readingTime":1.64,"hasTruncateMarker":true,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","imageURL":"https://github.com/im-naren.png","key":"narendra","page":null}],"frontMatter":{"slug":"message-queue","title":"Message Queue","authors":["narendra"],"tags":["big-data","kafka","message-queue","distributed-systems"],"date":"2016-09-20T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Zookeeper Introduction to Zookeeper","permalink":"/blog/zookeeper-introduction-to-zookeeper"},"nextItem":{"title":"Normalisation","permalink":"/blog/normalisation"}},"content":"## What is Message Passing?\\n\\nMessage passing is a technique to enable inter-process communication (IPC), or for inter-thread communication within the same process communication between two distributed or non-distributed parallel processes in synchronous or asynchronous mode, The communications are completed by the sending of messages (functions, signals and data packets) to recipients.\\n\\n\x3c!-- truncate --\x3e\\n\\nMost widely used messaging Patterns are:\\n- request-response\\n- Messaging-Queue\\n- Publisher-Subscriber\\n- RPC(remote-procedure-call)\\n- push-pull\\n\\n## Request Response:\\n\\nThe program that plays the role of servicing requests is called server. Correspondingly, the program that sends requests to a server is called client. We use server or client to refer to the role played by a program. A program can act as both server and client at the same time. this is most widely used in world wide web. we have both synchronuous and asynchronous versions of this.\\n\\n![client-server](/img/client-server-1.png)\\n\\n## Message Queue:\\n\\nMessage queues provide and asynchronous Point-to-point communications protocol, which means the sender puts messages in a queue and continue its processing without receiving an immediate response. the receiver can reach out to the messaging queue for receiving the messages. Messages placed onto the queue are stored until the recipient retrieves them. Message queues have implicit or explicit limits on the size of data that may be transmitted in a single message and the number of messages that may remain outstanding on the queue.\\n\\n![message-queue-model](/img/message-queue-model.png)\\n\\n## Publisher-Subscriber:\\n\\nPublish\u2013subscribe is a sibling of the message queue paradigm, this paradigm the sender of the message is called Publisher, who send the message to a topic without the knowledge of who are the specific receivers, called subscribers, similarly the subscribers receives the messages only of there interest, the messages gets filtered based on the topic and content. the only subscriber interested the particular topic or attribute, the matching constraints defined by the subscriber.\\n\\n![pub-sub-model](/img/pub-sub-model.png)"},{"id":"normalisation","metadata":{"permalink":"/blog/normalisation","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2015-09-14-normalisation.md","source":"@site/blog/2015-09-14-normalisation.md","title":"Normalisation","description":"Normalisation is the process of eliminating the redundancy, minimising the use of null values and prevention of the loss of information by establishing relations and ensuring data integrity.","date":"2015-09-14T00:00:00.000Z","tags":[{"inline":true,"label":"sql","permalink":"/blog/tags/sql"},{"inline":true,"label":"database","permalink":"/blog/tags/database"},{"inline":true,"label":"normalization","permalink":"/blog/tags/normalization"},{"inline":true,"label":"design","permalink":"/blog/tags/design"}],"readingTime":3.55,"hasTruncateMarker":true,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","imageURL":"https://github.com/im-naren.png","key":"narendra","page":null}],"frontMatter":{"slug":"normalisation","title":"Normalisation","authors":["narendra"],"tags":["sql","database","normalization","design"],"date":"2015-09-14T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Message Queue","permalink":"/blog/message-queue"},"nextItem":{"title":"How to sort a HashMap on Values","permalink":"/blog/how-to-sort-a-hashmap-on-values"}},"content":"Normalisation is the process of eliminating the redundancy, minimising the use of null values and prevention of the loss of information by establishing relations and ensuring data integrity.\\n\\nData should only be stored once and avoid storing data that can be calculated from other data already held in the database. During the process of normalisation redundancy must be removed, but not at the expense of breaking data integrity rules.\\n\\nThe removal of redundancy helps to prevent insertion, deletion, and update errors, since the data is only available in one attribute of one table in the database.\\n\\n\x3c!-- truncate --\x3e\\n\\nIf redundancy exists in the database then problems can arise when the database is in normal operation:\\n\\n- When data is inserted the data must be duplicated correctly in all places where there is redundancy. For instance, if two tables exist for in a database, and both tables contain the employee name, then creating a new employee entry requires that both tables be updated with the employee name.\\n- When data is modified in the database, if the data being changed has redundancy, then all versions of the redundant data must be updated simultaneously. So in the employee example a change to the employee name must happen in both tables simultaneously.\\n\\n## Aims of Normalisation\\n\\n- Normalisation ensures that the database is structured in the best possible way.\\n- To achieve control over data redundancy. There should be no unnecessary duplication of data in different tables.\\n- To ensure data consistency. Where duplication is necessary the data is the same.\\n- To ensure tables have a flexible structure. E.g. number of classes taken or books borrowed should not be limited.\\n- To allow data in different tables can be used in complex queries.\\n\\n## Stages of Normalisation\\n\\n- First Normal Form 1NF\\n- Second Normal Form 2NF\\n- Third Normal Form 3NF\\n- Boyce-Codd Normal Form BCNF\\n\\n## First Normal Form: 1NF\\n\\nA table is in its first normal form if it contains no repeating attributes or groups of attributes. To convert data for unnormalised form to 1NF, simply convert any repeated attributes into part of the candidate key.\\n\\n![un-normalise](/img/un-normalise.png)\\n\\n![first-degree-normalisation](/img/first-degree-normalisation.png)\\n\\n- A relation is in 1NF if it contains no repeating groups\\n- To convert an unnormalised relation to 1NF either\\n- Flatten the table and change the primary key, or\\n- Decompose the relation into smaller relations, one for the repeating groups and one for the non-repeating groups.\\n- Remember to put the primary key from the original relation into both new relations.\\n- This option is liable to give the best results.\\n\\n## Second Normal Form: 2NF\\n\\nA table is in the second normal form if it\'s in the first normal form AND no column that is not part of the primary key is dependant only a portion of the primary key.\\n\\nThe concept of functional dependency in central to normalisation and, in particular, strongly related to 2NF\\n\\n![second-degree-normalisation](/img/second-degree-normalisation.png)\\n\\n- A relation is in 2NF if it contains no repeating groups and no partial key functional dependencies\\n- Rule: A relation in 1NF with a single key field must be in 2NF\\n- To convert a relation with partial functional dependencies to 2NF. create a set of new relations:\\n- One relation for the attributes that are fully dependent upon the key.\\n- One relation for each part of the key that has partially dependent attributes\\n\\n## Third Normal Form: 3NF\\n\\nA table is in the third normal form if it is the second normal form and there are no non-key columns dependant on other non-key columns that could not act as the primary key.\\n\\n![third-degree-normalisation](/img/third-degree-normalisation.png)\\n\\n- A relation is in 3NF if it contains no repeating groups, no partial functional dependencies, and no transitive functional dependencies\\n- To convert a relation with transitive functional dependencies to 3NF, remove the attributes involved in the transitive dependency and put them in a new relation\\n- Rule: A relation in 2NF with only one non-key attribute must be in 3NF\\n- In a normalised relation a non-key field must provide a fact about the key, the whole key and nothing but the key.\\n- Relations in 3NF are sufficient for most practical database design problems. However, 3NF does not guarantee that all anomalies have been removed."},{"id":"how-to-sort-a-hashmap-on-values","metadata":{"permalink":"/blog/how-to-sort-a-hashmap-on-values","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2015-05-25-how-to-sort-a-hashmap-on-values.md","source":"@site/blog/2015-05-25-how-to-sort-a-hashmap-on-values.md","title":"How to sort a HashMap on Values","description":"HashMap doesn\'t preserve any order by default. If order is required we need to sort it explicitly according to the requirement.","date":"2015-05-25T00:00:00.000Z","tags":[{"inline":true,"label":"java","permalink":"/blog/tags/java"},{"inline":true,"label":"coding","permalink":"/blog/tags/coding"},{"inline":true,"label":"data-structure","permalink":"/blog/tags/data-structure"},{"inline":true,"label":"algorithms","permalink":"/blog/tags/algorithms"}],"readingTime":1.17,"hasTruncateMarker":true,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","imageURL":"https://github.com/im-naren.png","key":"narendra","page":null}],"frontMatter":{"slug":"how-to-sort-a-hashmap-on-values","title":"How to sort a HashMap on Values","authors":["narendra"],"tags":["java","coding","data-structure","algorithms"],"date":"2015-05-25T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Normalisation","permalink":"/blog/normalisation"},"nextItem":{"title":"How to Implement Singly Linked List in Java","permalink":"/blog/how-to-implement-singly-linked-list-in-java"}},"content":"HashMap doesn\'t preserve any order by default. If order is required we need to sort it explicitly according to the requirement.\\n\\nIn this article I have tried to explain how to sort a HasMap based on values.\\n\\n\x3c!-- truncate --\x3e\\n\\n## HasMap sorting by Value\\n\\nin this example I have used TreeMap to sort the HashMap. unlike a HashMap, a TreeMap guarantees that its elements will be sorted in ascending key order.\\n\\n### SortHashMap.java\\n\\n```java\\nimport java.util.Comparator;\\nimport java.util.HashMap;\\nimport java.util.Map;\\nimport java.util.TreeMap;\\n\\npublic class SortHashMap {\\n\\n    public static void main(String[] args) {\\n\\n        HashMap&lt;String,Integer&gt; map = new HashMap&lt;String,Integer&gt;();\\n        ValueComparator vcmp =  new ValueComparator(map);\\n        TreeMap&lt;String,Integer&gt; sorted_map = new TreeMap&lt;String,Integer&gt;(vcmp);\\n\\n        map.put(\\"naren\\",96);\\n        map.put(\\"ram\\",97);\\n        map.put(\\"manish\\",970);\\n        map.put(\\"kumar\\",97);\\n        map.put(\\"shruti\\",97);\\n        map.put(\\"rohit\\",760);\\n        map.put(\\"vatsal\\",444);\\n\\n        System.out.println(\\"unsorted map: \\"+map);\\n\\n        sorted_map.putAll(map);\\n\\n        System.out.println(\\"results: \\"+sorted_map);\\n    }\\n}\\n\\nclass ValueComparator implements Comparator<String> {\\n\\n    Map&lt;String, Integer&gt; base;\\n    public ValueComparator(Map&lt;String, Integer&gt; base) {\\n        this.base = base;\\n    }\\n\\n    // Note: this comparator imposes orderings that are inconsistent with equals.    \\n    public int compare(String a, String b) {\\n        if (base.get(a) >= base.get(b)) {\\n            return -1;\\n        } else {\\n            return 1;\\n        } // returning 0 would merge keys\\n    }\\n}\\n```\\n\\n### Output\\n\\n```java\\nunsorted map: {manish=970, kumar=97, ram=97, rohit=760, shruti=97, vatsal=444, naren=96}\\nresults: {manish=970, rohit=760, vatsal=444, shruti=97, ram=97, kumar=97, naren=96}\\n```"},{"id":"how-to-implement-singly-linked-list-in-java","metadata":{"permalink":"/blog/how-to-implement-singly-linked-list-in-java","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2015-05-15-how-to-implement-singly-linked-list-in-java.md","source":"@site/blog/2015-05-15-how-to-implement-singly-linked-list-in-java.md","title":"How to Implement Singly Linked List in Java","description":"A Linked List is a dynamic data structure. The number of nodes in a list is not fixed and can grow and shrink on demand. Any application which has to deal with an unknown number of objects will need to use a linked list.","date":"2015-05-15T00:00:00.000Z","tags":[{"inline":true,"label":"java","permalink":"/blog/tags/java"},{"inline":true,"label":"coding","permalink":"/blog/tags/coding"},{"inline":true,"label":"data-structure","permalink":"/blog/tags/data-structure"},{"inline":true,"label":"algorithms","permalink":"/blog/tags/algorithms"}],"readingTime":2.31,"hasTruncateMarker":true,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","imageURL":"https://github.com/im-naren.png","key":"narendra","page":null}],"frontMatter":{"slug":"how-to-implement-singly-linked-list-in-java","title":"How to Implement Singly Linked List in Java","authors":["narendra"],"tags":["java","coding","data-structure","algorithms"],"date":"2015-05-15T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"How to sort a HashMap on Values","permalink":"/blog/how-to-sort-a-hashmap-on-values"},"nextItem":{"title":"Introduction To MapReduce","permalink":"/blog/introduction-to-mapreduce"}},"content":"A **Linked List** is a dynamic data structure. The number of nodes in a list is not fixed and can grow and shrink on demand. Any application which has to deal with an unknown number of objects will need to use a linked list.\\n\\nLinked lists and arrays are similar since they both store collections of data. The terminology is that arrays and linked lists store \\"elements\\" on behalf of \\"client\\" code. The specific type of element is not important since essentially the same structure works to store elements of any type. The size of the array is fixed where Linked List is a dynamic and also Inserting new elements at the front or in middle of an array is potentially expensive because existing elements need to be shifted over to make room.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Types of Linked List\\n\\n- Singly Linked List\\n- Doubly Linked List\\n- Circular Linked List\\n\\nSingly Linked List is the most basic types of Linked List where every node holds some data and reference to next Node.\\n\\n![linkedlist](/img/linkedlist.png)\\n\\n## Implementation of Singly Linked List\\n\\n### Node.java\\n\\n```java\\npublic class Node {\\n    \\n    private Node nextNode;\\n    private Object data;\\n\\n    public Node(Object data){ this.data = data;}\\n    public Node(Object data, Node nextNode){ this.data=data;this.nextNode=nextNode;}\\n    public Object getData(){return data;}\\n    public void setData(Object data){this.data=data;}\\n    public Node getNextNode(){return nextNode;}\\n    public void setNextNode(Node nextNode){this.nextNode=nextNode;} \\n}\\n```\\n\\n### LinkedList.java\\n\\n```java\\npublic class LinkedList {\\n    \\n    private int listCount;\\n    private Node head;\\n\\n    public LinkedList() {\\n        head = new Node(null);\\n        listCount = 0;\\n    }\\n\\n    public void add(Object data) {\\n        Node newNode = new Node(data);\\n        Node currentNode = head;\\n        while (currentNode.getNextNode() != null) {\\n            currentNode = currentNode.getNextNode();\\n        }\\n        currentNode.setNextNode(newNode);\\n        listCount++;\\n    }\\n\\n    public void add(Object data, int index) {\\n\\n        if (index <= listCount && index > 0) {\\n            Node newNode = new Node(data);\\n            Node currentNode = head;\\n\\n            for (int i = 1; i < index; i++) {\\n                currentNode = currentNode.getNextNode();\\n            }\\n\\n            newNode.setNextNode(currentNode.getNextNode());\\n            currentNode.setNextNode(newNode);\\n            listCount++;\\n        }\\n    }\\n\\n    public Node get(int index) {\\n\\n        if (index > listCount || index <= 0)\\n            return null;\\n\\n        Node currentNode = head;\\n        for (int i = 1; i <= index; i++) {\\n            currentNode = currentNode.getNextNode();\\n        }\\n        return currentNode;\\n    }\\n\\n    public boolean remove(int index) {\\n\\n        if (index > listCount || index <= 0)\\n            return false;\\n\\n        Node currentNode = head;\\n        for (int i = 0; i < index - 2; i++) {\\n        currentNode = currentNode.getNextNode();\\n        }\\n\\n        currentNode.setNextNode(currentNode.getNextNode().getNextNode());\\n        listCount--;\\n        return true;\\n    }\\n     \\n    public int size() {\\n        return listCount;\\n    }\\n}\\n```\\n\\n### MainClass.java\\n\\n```java\\npublic class testClass {\\n \\n    public static void main(String[] args) {\\n        LinkedList lnklist = new LinkedList();\\n        lnklist.add(\\"naren\\");\\n        lnklist.add(\\"manish\\");\\n        lnklist.add(\\"ram\\");\\n        System.out.println(\\"Number Of Nodes in the List: \\" + lnklist.size());\\n\\n        lnklist.add(\\"naren1\\", 1);\\n        lnklist.add(\\"manish1\\", 3);\\n        lnklist.add(\\"ram1\\", 5);\\n        System.out.println(\\"Number Of Nodes in the List: \\" + lnklist.size());\\n\\n        System.out.println(\\"All the Nodes available in the List : \\");\\n        DisplayList(lnklist);\\n\\n        lnklist.remove(1);\\n        lnklist.remove(3);\\n        lnklist.remove(6);\\n        System.out.println(\\"Number Of Nodes in the List: \\" + lnklist.size());\\n        System.out.println(\\"All the Nodes available in the List : \\");\\n        DisplayList(lnklist);\\n    }\\n \\n    public static void DisplayList(LinkedList ll) {\\n        for (int i = 1; i <= ll.size(); i++) {\\n            System.out.println(i + \\" : \\" + ll.get(i).getData());\\n        }\\n    }\\n}\\n```"},{"id":"introduction-to-mapreduce","metadata":{"permalink":"/blog/introduction-to-mapreduce","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2015-05-14-introduction-to-mapreduce.md","source":"@site/blog/2015-05-14-introduction-to-mapreduce.md","title":"Introduction To MapReduce","description":"MapReduce is a framework for processing large amount of data residing on hundreds of computers, its an extraordinarily powerful paradigm. MapReduce was first introduced by Google in 2004 MapReduce: Simplified Data Processing on Large Clusters.","date":"2015-05-14T00:00:00.000Z","tags":[{"inline":true,"label":"hadoop","permalink":"/blog/tags/hadoop"},{"inline":true,"label":"mapreduce","permalink":"/blog/tags/mapreduce"},{"inline":true,"label":"big-data","permalink":"/blog/tags/big-data"},{"inline":true,"label":"java","permalink":"/blog/tags/java"}],"readingTime":1.75,"hasTruncateMarker":true,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","imageURL":"https://github.com/im-naren.png","key":"narendra","page":null}],"frontMatter":{"slug":"introduction-to-mapreduce","title":"Introduction To MapReduce","authors":["narendra"],"tags":["hadoop","mapreduce","big-data","java"],"date":"2015-05-14T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"How to Implement Singly Linked List in Java","permalink":"/blog/how-to-implement-singly-linked-list-in-java"},"nextItem":{"title":"HDFS Architecture","permalink":"/blog/hdfs-architecture"}},"content":"**MapReduce** is a framework for processing large amount of data residing on hundreds of computers, its an extraordinarily powerful paradigm. MapReduce was first introduced by Google in 2004 MapReduce: Simplified Data Processing on Large Clusters.\\n\\nIn this article we\'ll see how MapReduce processes the data, I am considering the Word Count program as a example, yeah!! this is the worlds most famous MapReduce program!!\\n\\n\x3c!-- truncate --\x3e\\n\\n## Overview\\n\\nThe MapReduce framework operates exclusively on *&lt;key, value&gt;* pairs, that is, the framework views the input to the job as a set of *&lt;key, value&gt;* pairs and produces a set of *&lt;key, value&gt;* pairs as the output of the job, conceivably of different types.\\n\\nThe key and value classes have to be serializable by the framework and hence need to implement the Writable interface. Additionally, the key classes have to implement the WritableComparable interface to facilitate sorting by the framework.\\n\\n## Input and Output types of a MapReduce job:\\n\\n*(input) &lt;k1, v1&gt; -> **map** -> &lt;k2, v2&gt; -> **combine** -> &lt;k2, v2&gt; -> **reduce** -> &lt;k3, v3&gt; (output)*\\n\\n## WordCount.java\\n\\n```java\\nimport java.io.IOException;\\nimport java.util.*;\\n \\nimport org.apache.hadoop.fs.Path;\\nimport org.apache.hadoop.conf.*;\\nimport org.apache.hadoop.io.*;\\nimport org.apache.hadoop.mapred.*;\\nimport org.apache.hadoop.util.*;\\n \\npublic class WordCount {\\n  public static class Map extends MapReduceBase implements Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {\\n    private final static IntWritable one = new IntWritable(1);\\n    private Text word = new Text();\\n  \\n    public void map(LongWritable key, Text value, \\n                    OutputCollector&lt;Text, IntWritable&gt; output, \\n                    Reporter reporter) throws IOException {\\n      String line = value.toString();\\n      StringTokenizer tokenizer = new StringTokenizer(line);\\n      while (tokenizer.hasMoreTokens()) {\\n        word.set(tokenizer.nextToken());\\n        output.collect(word, one);\\n      }\\n    }\\n  }\\n  \\n  public static class Reduce extends MapReduceBase implements Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {\\n    public void reduce(Text key, Iterator&lt;IntWritable&gt; values, \\n                        OutputCollector&lt;Text, IntWritable&gt; output, \\n                        Reporter reporter) throws IOException {\\n      int sum = 0;\\n      while (values.hasNext()) {\\n        sum += values.next().get();\\n      }\\n      output.collect(key, new IntWritable(sum));\\n    }\\n  }\\n  \\n  public static void main(String[] args) throws Exception {\\n    JobConf conf = new JobConf(WordCount.class);\\n    conf.setJobName(\\"wordcount\\");\\n    \\n    conf.setOutputKeyClass(Text.class);\\n    conf.setOutputValueClass(IntWritable.class);\\n    \\n    conf.setMapperClass(Map.class);\\n    conf.setCombinerClass(Reduce.class);\\n    conf.setReducerClass(Reduce.class);\\n    \\n    conf.setInputFormat(TextInputFormat.class);\\n    conf.setOutputFormat(TextOutputFormat.class);\\n    \\n    FileInputFormat.setInputPaths(conf, new Path(args[0]));\\n    FileOutputFormat.setOutputPath(conf, new Path(args[1]));\\n    \\n    JobClient.runJob(conf);\\n  }\\n}\\n```"},{"id":"hdfs-architecture","metadata":{"permalink":"/blog/hdfs-architecture","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2015-05-03-hdfs-architecture.md","source":"@site/blog/2015-05-03-hdfs-architecture.md","title":"HDFS Architecture","description":"The Hadoop Distributed File System (HDFS) is a highly fault tolerant file system designed and optimized to be deployed on a distributed infrastructure established with a bunch commodity hardware. HDFS provides high throughput access to application data and is best suited for applications that have large data sets. Unlike existing distributed file systems HDFS have loosen up a few POSIX Standards to enable streaming access to file system data. HDFS was originally developed as an infrastructure for the Apache Nutch web search engine project.","date":"2015-05-03T00:00:00.000Z","tags":[{"inline":true,"label":"hadoop","permalink":"/blog/tags/hadoop"},{"inline":true,"label":"hdfs","permalink":"/blog/tags/hdfs"},{"inline":true,"label":"big-data","permalink":"/blog/tags/big-data"},{"inline":true,"label":"architecture","permalink":"/blog/tags/architecture"}],"readingTime":1.94,"hasTruncateMarker":true,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","imageURL":"https://github.com/im-naren.png","key":"narendra","page":null}],"frontMatter":{"slug":"hdfs-architecture","title":"HDFS Architecture","authors":["narendra"],"tags":["hadoop","hdfs","big-data","architecture"],"date":"2015-05-03T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Introduction To MapReduce","permalink":"/blog/introduction-to-mapreduce"},"nextItem":{"title":"MapReduce Execution in Hadoop","permalink":"/blog/mapreduce-execution-in-hadoop"}},"content":"The Hadoop Distributed File System (HDFS) is a highly fault tolerant file system designed and optimized to be deployed on a distributed infrastructure established with a bunch commodity hardware. HDFS provides high throughput access to application data and is best suited for applications that have large data sets. Unlike existing distributed file systems HDFS have loosen up a few POSIX Standards to enable streaming access to file system data. HDFS was originally developed as an infrastructure for the Apache Nutch web search engine project.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Inside HDFS\\n\\n![hdfs-architecture.png](/img/hdfs-architecture.png)\\n\\nHDFS is based on Master-Slave Architecture. A typical HDFS cluster consists one NameNode and multiple DataNodes generally one per node in the cluster. The NameNode is the arbitrator and repository for all **HDFS metadata**. The system is designed in such a way that user data never flows through the NameNode. NameNode acts as the master in Master-Slave Architectural pattern and manages the file system namespace and regulates access to files by client Applications. DataNode manages the user data stored on the node that they run on. A file is split into one or more blocks and set of blocks are stored in DataNodes. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes from the heartbeats and block reports sent by DataNodes. DataNodes executes read, write requests and performs block creation, block deletion, and block replication only when NameNode commands them to.\\n\\n## Deployment\\n\\nNameNode and DataNode are software programs designed in java, so it can execute on any machine having java installed (typically Unix/Linux), it can be deployed on a wide range of machines. In real time scenarios NameNodes are deployed on a high end dedicated machines and all other machines in the cluster are commodity hardware running DataNode program. Each cluster consists of one NameNode and multiple DataNodes. There could be multiple NameNodes within a cluster in some special scenarios, In order to scale name services horizontally federation uses multiple independent NameNodes within a cluster these NameNodes does not require coordination between each other. The DataNodes are used as common storage among the NameNodes, and all the DataNodes are required to be registered with all the NameNodes and send periodic heartbeats and block reports to the NameNodes."},{"id":"mapreduce-execution-in-hadoop","metadata":{"permalink":"/blog/mapreduce-execution-in-hadoop","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2015-03-10-mapreduce-execution-in-hadoop.md","source":"@site/blog/2015-03-10-mapreduce-execution-in-hadoop.md","title":"MapReduce Execution in Hadoop","description":"In this article we have tried to summaries,  how a MapReduce program executes in Hadoop environment.","date":"2015-03-10T00:00:00.000Z","tags":[{"inline":true,"label":"hadoop","permalink":"/blog/tags/hadoop"},{"inline":true,"label":"mapreduce","permalink":"/blog/tags/mapreduce"},{"inline":true,"label":"big-data","permalink":"/blog/tags/big-data"}],"readingTime":1.08,"hasTruncateMarker":true,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","imageURL":"https://github.com/im-naren.png","key":"narendra","page":null}],"frontMatter":{"slug":"mapreduce-execution-in-hadoop","title":"MapReduce Execution in Hadoop","authors":["narendra"],"tags":["hadoop","mapreduce","big-data"],"date":"2015-03-10T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"HDFS Architecture","permalink":"/blog/hdfs-architecture"},"nextItem":{"title":"Difference between Unix and Linux","permalink":"/blog/difference-between-unix-and-linux"}},"content":"In this article we have tried to summaries,  how a MapReduce program executes in Hadoop environment.\\n\\n\x3c!-- truncate --\x3e\\n\\n## MapReduce 1 Execution Sequence:\\n\\nMapReduce execution starts with below command.\\n\\n**Step 1:** `$ hadoop jar <jar> [mainClass] args...`\\n\\nThis command starts the MapReduce execution in Clients JVM.\\ncreates the Job.\\n\\n**Step 2:** `JobTracker.getNewJobId()`\\n\\nClient Asks JobTracker for a new JobId.\\n\\n**Step 3:** `JobClient.SubmitJob() / JobClient.runJob()`\\n\\n- Checking the input and output specifications of the job.\\n- Computing the InputSplits for the job.\\n- Setup the requisite accounting information for the DistributedCache of the job, if necessary.\\n- Copying the job\'s jar and configuration to the JobTracker  file-system, in a folder names as the JobId assigned with very high replication factor(default 10).\\n- Submitting the job to the JobTracker and optionally monitoring it\'s status.\\n- JobTracker puts the job into an internal queue from where JobScheduler will pick it up and Initialize\\n\\n**Step 4:** Initialize the Job\\n\\n- Creates an object of the Job.\\n- Encapsulates its Tasks.\\n- Retrieve the InputSplit and create one map task for each split.\\n- Creates the reduces task the number of reduce task depends on the number defined in the driver code(default 1).\\n- Creates a Job Setup task to setup the job before map tasks run.\\n- Creates a Job Cleanup task to run after the reducer task run.\\n\\n\x3c!-- Image removed: mr1_execution.png --\x3e"},{"id":"difference-between-unix-and-linux","metadata":{"permalink":"/blog/difference-between-unix-and-linux","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2015-03-07-difference-between-unix-and-linux.md","source":"@site/blog/2015-03-07-difference-between-unix-and-linux.md","title":"Difference between Unix and Linux","description":"Back in 1969, UNIX has evolved through a number of different versions and environment. One of the Original UNIX editors has licensed the most modern UNIX variants. It\'s closed Source software. There are various flavors of UNIX are available in market like Sun\'s Solaris, Hewlett-Packard\'s HP-UX, and IBM\'s AIX all of these have their own Unique Foundation, all these flavors are optimized and incorporated with different tools which are most compatible with their hardware.","date":"2015-03-07T00:00:00.000Z","tags":[{"inline":true,"label":"unix","permalink":"/blog/tags/unix"},{"inline":true,"label":"linux","permalink":"/blog/tags/linux"},{"inline":true,"label":"operating-systems","permalink":"/blog/tags/operating-systems"},{"inline":true,"label":"hadoop","permalink":"/blog/tags/hadoop"}],"readingTime":2.54,"hasTruncateMarker":true,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","imageURL":"https://github.com/im-naren.png","key":"narendra","page":null}],"frontMatter":{"slug":"difference-between-unix-and-linux","title":"Difference between Unix and Linux","authors":["narendra"],"tags":["unix","linux","operating-systems","hadoop"],"date":"2015-03-07T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"MapReduce Execution in Hadoop","permalink":"/blog/mapreduce-execution-in-hadoop"},"nextItem":{"title":"How to Find out Next and Previous Day of Week in Oracle","permalink":"/blog/how-to-find-out-next-and-previous-day-of-week-in-oracle"}},"content":"Back in 1969, UNIX has evolved through a number of different versions and environment. One of the Original UNIX editors has licensed the most modern UNIX variants. It\'s closed Source software. There are various flavors of UNIX are available in market like Sun\'s Solaris, Hewlett-Packard\'s HP-UX, and IBM\'s AIX all of these have their own Unique Foundation, all these flavors are optimized and incorporated with different tools which are most compatible with their hardware.\\n\\nLinux is an open source (free to use and redistribute under GNU licenses) operating system widely used for computer hardware and software, game development, tablet PCS, mainframes etc. UNIX is a copyrighted name only big companies (IBM, HP etc\u2026) are allowed to use the UNIX copyright and names. UNIX is commonly used in internet servers, workstations and PCs by Solaris, Intel, and HP etc.\\n\\n\x3c!-- truncate --\x3e\\n\\nLinux Kernel was designed by Linus Torvalds with an intention of developing a Unix-like operating system as an open source alternative of UNIX environment.  An administrator or developer who supports Linux systems might find it uncomfortable to move to a commercial UNIX system. On the whole, the foundations of any UNIX-like operating system (tools, filesystem layout, programming APIs) are fairly standardized. However, some details of the systems show significant differences. The remainder of this article covers the details of these differences.\\n\\n## In The Box\\n\\nLinux is designed as just kernel. All Linux distributions package includes GUI system, GNU utilities, installation & management tools, GNU C/C++ Compilers, Editors and various applications (e.g. OpenOffice, Firefox). However, most UNIX operating systems come as A-Z package everything is designed and distributed by the same vender.\\n\\n## Interface\\n\\nLinux is considered as most user friendly UNIX like operating systems. It makes it easy to install sound card, flash players, and other desktop goodies. However, Apple OS X is most popular UNIX operating system for desktop usage.\\n\\n## Filesystem support\\n\\nOne of the reasons Linux has become such a powerful tool is its immense support for other operating systems. One of the most popular features is the plethora of filesystems that are available. Most commercial version of UNIX supports two, or possibly three, different local filesystem types. Linux, however, supports almost all of the filesystems that are currently available on any operating system.\\n\\n## Usages\\n\\nLinux can be installed on a wide variety of computer hardware, ranging from mobile phones, tablet computers and video game consoles, to mainframes and supercomputers where as The UNIX operating system is used in internet servers, workstations & PCs. Backbone of the majority of finance infrastructure and many 24\xd7365 high availability solutions.\\n\\n## In Short\\n\\nClearly the general environment between UNIX and Linux is very similar. However they have differences in their filesystem and Kernel, but overall these differences are not major. They might need specialization in optimizing the work, but since the underlying concepts are so mature that changes are very rare. There are so many differences in UNIX itself among its variants that UNIX and Linux feel like they are one of the variant of each other."},{"id":"how-to-find-out-next-and-previous-day-of-week-in-oracle","metadata":{"permalink":"/blog/how-to-find-out-next-and-previous-day-of-week-in-oracle","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2015-02-03-how-to-find-out-next-and-previous-day-of-week-in-oracle.md","source":"@site/blog/2015-02-03-how-to-find-out-next-and-previous-day-of-week-in-oracle.md","title":"How to Find out Next and Previous Day of Week in Oracle","description":"I have seen people writing number of lines of code to find out the date on a specific day in current or previous week, most often we need Friday the last working day of the week. We have a pretty simple way to find out in Oracle Queries.","date":"2015-02-03T00:00:00.000Z","tags":[{"inline":true,"label":"oracle","permalink":"/blog/tags/oracle"},{"inline":true,"label":"sql","permalink":"/blog/tags/sql"},{"inline":true,"label":"database","permalink":"/blog/tags/database"},{"inline":true,"label":"hadoop","permalink":"/blog/tags/hadoop"}],"readingTime":1.03,"hasTruncateMarker":true,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","imageURL":"https://github.com/im-naren.png","key":"narendra","page":null}],"frontMatter":{"slug":"how-to-find-out-next-and-previous-day-of-week-in-oracle","title":"How to Find out Next and Previous Day of Week in Oracle","authors":["narendra"],"tags":["oracle","sql","database","hadoop"],"date":"2015-02-03T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Difference between Unix and Linux","permalink":"/blog/difference-between-unix-and-linux"}},"content":"I have seen people writing number of lines of code to find out the date on a specific day in current or previous week, most often we need Friday the last working day of the week. We have a pretty simple way to find out in Oracle Queries.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Logic:\\n- Add a number of the day you want the date for in your current date, assuming Monday as day 1, Tuesday as day 2 and so on.\\n\\n```sql\\nTRUNC(TO_DATE(SYSDATE,\'YYYYMMDD\') + 1, \'d\')\\n```\\n\\nSince Oracle considers Sunday as the first day of the week and Saturday as the last day adding 1 will shift this week frame one day back now week starts from Saturday and ends on Friday and TRUNC will give the currents weeks Sunday.\\n\\n- Now whatever day you want in the week just add or subtract the number of day from the Sunday you got. In our case we are looking for Friday so we add 5 since it is the 5 day of our new week.\\n\\n```sql\\nTRUNC(TO_DATE(SYSDATE,\'YYYYMMDD\') + 1, \'d\') + 5\\n```\\n\\nSo our Query will look something like this\\n\\n```sql\\nSELECT TRUNC(TO_DATE(SYSDATE,\'YYYYMMDD\') + 1, \'d\') + 52\\nFROM DUAL;\\n```\\n\\n**Output:**\\n\\n\x3c!-- Image removed: previous-day-of-week-sql.png --\x3e"}]}}')}}]);