"use strict";(self.webpackChunkmy_blog=self.webpackChunkmy_blog||[]).push([[177],{3246:(e,t,a)=>{a.d(t,{A:()=>s});const s=a.p+"assets/images/hdfs-architecture-c010c28d8c61cd561cd6b49cb433bc23.png"},4877:e=>{e.exports=JSON.parse('{"permalink":"/develbyte/blog/hdfs-architecture","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2015-05-03-hdfs-architecture.md","source":"@site/blog/2015-05-03-hdfs-architecture.md","title":"HDFS Architecture","description":"The Hadoop Distributed File System (HDFS) is a highly fault tolerant file system designed and optimized to be deployed on a distributed infrastructure established with a bunch commodity hardware. HDFS provides high throughput access to application data and is best suited for applications that have large data sets. Unlike existing distributed file systems HDFS have loosen up a few POSIX Standards to enable streaming access to file system data. HDFS was originally developed as an infrastructure for the Apache Nutch web search engine project.","date":"2015-05-03T00:00:00.000Z","tags":[{"inline":false,"label":"Hadoop","permalink":"/develbyte/blog/tags/hadoop","description":"Apache Hadoop ecosystem"},{"inline":false,"label":"HDFS","permalink":"/develbyte/blog/tags/hdfs","description":"Hadoop Distributed File System"},{"inline":false,"label":"Big Data","permalink":"/develbyte/blog/tags/big-data","description":"Big data technologies and concepts"},{"inline":false,"label":"Architecture","permalink":"/develbyte/blog/tags/architecture","description":"System and software architecture"}],"readingTime":1.93,"hasTruncateMarker":false,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","page":{"permalink":"/develbyte/blog/authors/narendra"},"socials":{"github":"https://github.com/im-naren","linkedin":"https://www.linkedin.com/in/im-naren/","email":"mailto:naren.dubey@zoho.com"},"imageURL":"https://github.com/im-naren.png","key":"narendra"}],"frontMatter":{"slug":"hdfs-architecture","title":"HDFS Architecture","authors":["narendra"],"tags":["hadoop","hdfs","big-data","architecture"],"date":"2015-05-03T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Introduction To MapReduce","permalink":"/develbyte/blog/introduction-to-mapreduce"},"nextItem":{"title":"MapReduce Execution in Hadoop","permalink":"/develbyte/blog/mapreduce-execution-in-hadoop"}}')},6614:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>l});var s=a(4877),n=a(4848),i=a(8453);const o={slug:"hdfs-architecture",title:"HDFS Architecture",authors:["narendra"],tags:["hadoop","hdfs","big-data","architecture"],date:new Date("2015-05-03T00:00:00.000Z")},r="HDFS Architecture",d={authorsImageUrls:[void 0]},l=[{value:"Inside HDFS",id:"inside-hdfs",level:2},{value:"Deployment",id:"deployment",level:2}];function c(e){const t={h2:"h2",img:"img",p:"p",strong:"strong",...(0,i.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.p,{children:"The Hadoop Distributed File System (HDFS) is a highly fault tolerant file system designed and optimized to be deployed on a distributed infrastructure established with a bunch commodity hardware. HDFS provides high throughput access to application data and is best suited for applications that have large data sets. Unlike existing distributed file systems HDFS have loosen up a few POSIX Standards to enable streaming access to file system data. HDFS was originally developed as an infrastructure for the Apache Nutch web search engine project."}),"\n",(0,n.jsx)(t.h2,{id:"inside-hdfs",children:"Inside HDFS"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"hdfs-architecture.png",src:a(3246).A+"",width:"520",height:"516"})}),"\n",(0,n.jsxs)(t.p,{children:["HDFS is based on Master-Slave Architecture. A typical HDFS cluster consists one NameNode and multiple DataNodes generally one per node in the cluster. The NameNode is the arbitrator and repository for all ",(0,n.jsx)(t.strong,{children:"HDFS metadata"}),". The system is designed in such a way that user data never flows through the NameNode. NameNode acts as the master in Master-Slave Architectural pattern and manages the file system namespace and regulates access to files by client Applications. DataNode manages the user data stored on the node that they run on. A file is split into one or more blocks and set of blocks are stored in DataNodes. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes from the heartbeats and block reports sent by DataNodes. DataNodes executes read, write requests and performs block creation, block deletion, and block replication only when NameNode commands them to."]}),"\n",(0,n.jsx)(t.h2,{id:"deployment",children:"Deployment"}),"\n",(0,n.jsx)(t.p,{children:"NameNode and DataNode are software programs designed in java, so it can execute on any machine having java installed (typically Unix/Linux), it can be deployed on a wide range of machines. In real time scenarios NameNodes are deployed on a high end dedicated machines and all other machines in the cluster are commodity hardware running DataNode program. Each cluster consists of one NameNode and multiple DataNodes. There could be multiple NameNodes within a cluster in some special scenarios, In order to scale name services horizontally federation uses multiple independent NameNodes within a cluster these NameNodes does not require coordination between each other. The DataNodes are used as common storage among the NameNodes, and all the DataNodes are required to be registered with all the NameNodes and send periodic heartbeats and block reports to the NameNodes."})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},8453:(e,t,a)=>{a.d(t,{R:()=>o,x:()=>r});var s=a(6540);const n={},i=s.createContext(n);function o(e){const t=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:o(e.components),s.createElement(i.Provider,{value:t},e.children)}}}]);