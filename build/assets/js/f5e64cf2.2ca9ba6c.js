"use strict";(self.webpackChunkmy_blog=self.webpackChunkmy_blog||[]).push([[6754],{2354:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/mr1_execution-17aa8aab15b4c2e0a2e32d5e7bf30c65.png"},4156:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=t(5615),a=t(4848),o=t(8453);const r={slug:"mapreduce-execution-in-hadoop",title:"MapReduce Execution in Hadoop",authors:["narendra"],tags:["hadoop","mapreduce","big-data"],date:new Date("2015-03-10T00:00:00.000Z")},s="MapReduce Execution in Hadoop",c={authorsImageUrls:[void 0]},d=[{value:"MapReduce 1 Execution Sequence:",id:"mapreduce-1-execution-sequence",level:2}];function l(e){const n={code:"code",h2:"h2",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.p,{children:"In this article we have tried to summaries,  how a MapReduce program executes in Hadoop environment."}),"\n",(0,a.jsx)(n.h2,{id:"mapreduce-1-execution-sequence",children:"MapReduce 1 Execution Sequence:"}),"\n",(0,a.jsx)(n.p,{children:"MapReduce execution starts with below command."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Step 1:"})," ",(0,a.jsx)(n.code,{children:"$ hadoop jar <jar> [mainClass] args..."})]}),"\n",(0,a.jsx)(n.p,{children:"This command starts the MapReduce execution in Clients JVM.\ncreates the Job."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Step 2:"})," ",(0,a.jsx)(n.code,{children:"JobTracker.getNewJobId()"})]}),"\n",(0,a.jsx)(n.p,{children:"Client Asks JobTracker for a new JobId."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Step 3:"})," ",(0,a.jsx)(n.code,{children:"JobClient.SubmitJob() / JobClient.runJob()"})]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Checking the input and output specifications of the job."}),"\n",(0,a.jsx)(n.li,{children:"Computing the InputSplits for the job."}),"\n",(0,a.jsx)(n.li,{children:"Setup the requisite accounting information for the DistributedCache of the job, if necessary."}),"\n",(0,a.jsx)(n.li,{children:"Copying the job's jar and configuration to the JobTracker  file-system, in a folder names as the JobId assigned with very high replication factor(default 10)."}),"\n",(0,a.jsx)(n.li,{children:"Submitting the job to the JobTracker and optionally monitoring it's status."}),"\n",(0,a.jsx)(n.li,{children:"JobTracker puts the job into an internal queue from where JobScheduler will pick it up and Initialize"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Step 4:"})," Initialize the Job"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Creates an object of the Job."}),"\n",(0,a.jsx)(n.li,{children:"Encapsulates its Tasks."}),"\n",(0,a.jsx)(n.li,{children:"Retrieve the InputSplit and create one map task for each split."}),"\n",(0,a.jsx)(n.li,{children:"Creates the reduces task the number of reduce task depends on the number defined in the driver code(default 1)."}),"\n",(0,a.jsx)(n.li,{children:"Creates a Job Setup task to setup the job before map tasks run."}),"\n",(0,a.jsx)(n.li,{children:"Creates a Job Cleanup task to run after the reducer task run."}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"mr1_execution",src:t(2354).A+"",width:"693",height:"577"})})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}},5615:e=>{e.exports=JSON.parse('{"permalink":"/blog/mapreduce-execution-in-hadoop","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2015-03-10-mapreduce-execution-in-hadoop.md","source":"@site/blog/2015-03-10-mapreduce-execution-in-hadoop.md","title":"MapReduce Execution in Hadoop","description":"In this article we have tried to summaries,  how a MapReduce program executes in Hadoop environment.","date":"2015-03-10T00:00:00.000Z","tags":[{"inline":false,"label":"Hadoop","permalink":"/blog/tags/hadoop","description":"Apache Hadoop ecosystem"},{"inline":false,"label":"MapReduce","permalink":"/blog/tags/mapreduce","description":"MapReduce programming model"},{"inline":false,"label":"Big Data","permalink":"/blog/tags/big-data","description":"Big data technologies and concepts"}],"readingTime":1.07,"hasTruncateMarker":false,"authors":[{"name":"Narendra Dubey","title":"Builder of Data Systems and Software","url":"https://im-naren.github.io/about/","page":{"permalink":"/blog/authors/narendra"},"socials":{"github":"https://github.com/im-naren","linkedin":"https://www.linkedin.com/in/im-naren/","email":"mailto:naren.dubey@zoho.com"},"imageURL":"https://github.com/im-naren.png","key":"narendra"}],"frontMatter":{"slug":"mapreduce-execution-in-hadoop","title":"MapReduce Execution in Hadoop","authors":["narendra"],"tags":["hadoop","mapreduce","big-data"],"date":"2015-03-10T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"HDFS Architecture","permalink":"/blog/hdfs-architecture"},"nextItem":{"title":"Difference between Unix and Linux","permalink":"/blog/difference-between-unix-and-linux"}}')},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var i=t(6540);const a={},o=i.createContext(a);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);